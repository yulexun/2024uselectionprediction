---
title: "Prediction of 2024 US election ..."
author: 
  - Colin Sihan Yang
  - Lexun Yu
  - Siddharth Gowda
thanks: "Code and data are available at: [https://github.com/yulexun/uselection](https://github.com/yulexun/uselection)."
date: today
date-format: long
abstract: "We forecast the winner of the 2024 US presidential election using “poll-of-polls” by building a linear model."
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(palmerpenguins)
library(here)
library(tibble)
library(knitr)
library(brms)
library(rstanarm)
library(gridExtra)
library(Matrix)
library(rstan)
library(brms)

```

```{r}
#| echo: false
#| warning: false
cleaned_data = read_csv(here("data/02-analysis_data/cleaned_data.csv"))
```

# Introduction {#sec-intro}
Election result forecasting has become an essential tool for analysts in political science and the public to predict the outcome of democratic process, such as the presidential election in the United States. Traditionally, individual polls have been used as a snapshot of voter sentiment, but they only reflect temporary changes in the performance of contestants, instead of a precise estimation of the election result. As discussed by @pasek and @blumenthal, the aggregation of multiple polls, or "poll-of-polls," has become a popular technique to reduce individual survey errors and provide more accurate election forecasts. However, the traditional poll aggregation does not reflect dynamics of an election, especially with real-time changes and the introduction of new data. This creates a gap for a more adaptable model to predict the election result based on both polling data and additional variables, such as historical data and economic indicators.

This paper fills the gap by building a hybrid election forecasting model following the strategies mentioned by @pasek. As @pasek described in their article, aggregation involves determining which surveys are worth including, as well as selecting, combining and averaging results from multiple polls to reduce individual biases and errors. Prediction modeling adds other data to the model that predicts election outcomes based on current dynamics. Hybrid models like the Bayesian approach incorporates prior beliefs based on historical data or expert knowledge and new evidence like economic updates to dynamically adjust the forecast as the campaign progresses. 

In this paper, we aim to predict the 2024 us election result with the hybrid election forcasting model. We incorporate aggregation by filtering the polls on @fivethirtyeight by numeric grade that indicates pollster’s reliability, prediction that incorporates social and economic indicators including unemployment rates and abortion rates, and hybrid approaches that leverages Bayesian techniques which combines historical data such as the 2016 election data, allowing for a dynamic prediction of the U.S. presidential election. 

The estimand for this research paper is the predicted support percentages for Kamala Harris and Donald Trump. The prediction is based on quantifying various polling factors, including sample size, poll scores, and transparency scores, which are used as predictors.

The results of this model indicate a more stable and accurate forecast compared to traditional aggregation methods alone, [update this …]

The remainder of this paper is structured as follows: [update this …]


# Data {#sec-data}

## Overview

For the data we used in this analysis about the polling result for Kamala Harris and Donalad Trump in 2024 USA president election. \
- **response variable:** `pct`(pct: The percentage of the vote or support that the candidate received in the poll) \
- **numeric predictor:**\
 `sample size`(sample_size: The total number of respondents participating in the poll) \
 `timegap`(the time gap between the poll start date and the real election date i.e timegap = real US election date - poll start date) \
 `pollscore`(A numeric value representing the score or reliability of the pollster in question) \
- **categorical predictor**
`state`(The U.S. state where the poll was conducted or focused) \
`methodology`(The method used to conduct the poll)\

## Data Exploration
```{r}
#| label: tbl-explore-data
#| echo: false
#| warning: false
a <- ggplot(data=cleaned_data, aes(x=sample_size, y=pct)) + 
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) + 
  labs(x = 'sample size', y='pct', 
       title = 'pct vs sample size')

b <- ggplot(data=cleaned_data, aes(x=pollscore, y=pct)) + 
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) + 
  labs(x = 'pollscore', y='pct', 
       title = 'pct vs pollscore')

c <- ggplot(data=cleaned_data, aes(x=methodology, y=pct)) + 
  geom_boxplot() + 
  labs(x = 'methodology', y='pct', 
       title = 'pct vs methodology')

d <- ggplot(data=cleaned_data, aes(x=days_taken_from_election, y=pct)) + 
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) + 
  labs(x = 'max gust speed', y='pct', 
       title = 'pct vs days taken from election')

e <- ggplot(data=cleaned_data, aes(x=state, y=pct)) + 
  geom_boxplot() + 
  labs(x = 'state', y='pct', 
       title = 'pct vs state')

grid.arrange(a, b, c, d, e, nrow=2)
```

- pct vs sample size: This scatter plot shows the pct against the sample size, with a fitted trend line indicating a slight positive relationship. The data points are denser for lower sample sizes, suggesting that smaller sample sizes are more common in the dataset.

- pct vs pollscore: This scatter plot illustrates the pct against pollscore. The fitted trend line suggests a weak negative relationship between pollscore and pct. The points are scattered without a strong linear pattern.

- pct vs methodology: A boxplot comparing pct for different polling methodologies. The pct distribution varies across methodologies, with some showing greater spread or median differences. This suggests that the polling methodology may influence pct outcomes.

- pct vs days taken from election: A scatter plot displaying pct versus the number of days before the election. The trend line indicates a slight negative relationship, suggesting that as the election date approaches, pct may decrease slightly.

- pct vs state: A boxplot depicting pct across different states. The pct distribution varies by state, with some states showing wider variability or different median values, implying state-specific effects on pct.



```{r}
#| label: fig-pairs
#| echo: false
#| warning: false
# numeric_data <- cleaned_data[sapply(cleaned_data, is.numeric)]
numeric_data = cleaned_data |> select(pct, sample_size, pollscore, days_taken_from_election)

# Create the pairs plot
pairs(numeric_data)
```

@fig-pairs The pairs plot displays scatter plots of four numeric variables (`pct`, `sample_size`, `pollscore`, and `days_taken_from_election`) to visualize their relationships. The data shows clustering, particularly in `pct` versus `sample_size`, suggesting potential heteroscedasticity. The `sample_size` variable is skewed towards lower values, while `pollscore` and `days_taken_from_election` have a more even spread, though `pollscore` shows central clustering. No strong linear relationships are immediately apparent between the variables, indicating that correlations are likely weak. 

## Measurement

In this dataset, each row represents a polling question that records the variables of interest. Each entry allows us to explore the real-world relationships between polling factors and the support percentage (`pct`) for the candidates Kamala Harris and Donald Trump. This dataset enables an analysis of how various polling characteristics influence the reported support levels for the candidates we are focused.


## Clean Data
@tbl-cleaned-data The data cleaning process involves several steps to ensure the quality and relevance of the polling data. First, we filter the dataset to retain only poll results with a numeric grade of 2.7 or higher, indicating that the polls are considered reliable. Next, we address missing values in the state attribute: polls with NA in the state column are considered national polls.

We then create a new attribute, days_taken_from_election, which represents the time gap between the poll's start date and the actual U.S. election date. Additionally, we filter the dataset to include only polls conducted after July 21, 2024, the date when Kamala Harris declared her candidacy. Finally, we remove any remaining rows that contain missing values to ensure a clean dataset.

```{r}
#| label: tbl-cleaned-data
#| tbl-cap: Sample of cleaned US election data 
#| echo: false
#| warning: false

cleaned_data |>
  select(pct, sample_size, pollscore, days_taken_from_election, state, methodology, candidate_name) |>
  head(6) |>
  kable(
    col.names = c("pct", "sample_size", "pollscore", "days_taken_from_election","state", "methodology", "candidate_name"),
    booktabs = TRUE
    ) 
```


## Basic Statistics Summary for Data

```{r}
#| label: fig-clean-data-histogram-Harris-Trump
#| fig-cap: the average PCT vs State for Harris and Trump
#| echo: false
#| warning: false
#| fig-subcap: ["average PCT vs State for Harris","PCT vs State for Trump"]
#| layout-nrow: 2
#| fig-width: 40
#| fig-height: 20
Harris_data = cleaned_data[cleaned_data$candidate_name == "Kamala Harris", ]
Harris_data |>
  group_by(state) |>
  summarise(avg_pct = mean(pct, na.rm = TRUE)) |>
  ggplot(mapping = aes(x = state, y = avg_pct)) + 
  geom_col()  +
  theme_minimal() +
  labs(
    x = "State", y = "support percentage(pct)", title = "average support percentage(pct) for Harris per state"
  )


Trump_data = cleaned_data[cleaned_data$candidate_name == "Donald Trump", ]
Trump_data |>
  group_by(state) |>
  summarise(avg_pct = mean(pct, na.rm = TRUE)) |>
  ggplot(mapping = aes(x = state, y = avg_pct)) + 
  geom_col()  +
  theme_minimal() +
  labs(
    x = "State", y = "support percentage(pct)", title = "average support percentage(pct) for Harris per state"
  )


# cleaned_data %>%
#   mutate(month = factor(month, levels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun",
#                                           "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"))) %>%
#   ggplot(mapping = aes(x = month, y = total_decedents)) + 
#   geom_bar(stat = "identity") +
#   theme_minimal() +
#   labs(
#     x = "Month", 
#     y = "Total Decedents"
#   )
```

\newpage

@fig-clean-data-histogram-Harris-Trump-1 The histogram displays the average support percentage for Kamala Harris across different U.S. states. The data indicates that support for Harris varies significantly across states. Notable observations include relatively high average support in states such as California and New York, which are known for being more Democratic-leaning. On the other hand, there are states with lower average support percentages, particularly in more traditionally Republican or swing states. The distribution suggests regional variations in support, with some states showing consistent backing for Harris while others indicate a weaker performance.

@fig-clean-data-histogram-Harris-Trump-2 The second histogram shows the average support percentage for Donald Trump across various states. It highlights substantial support in states such as Florida and Texas, which align with historical trends of strong Republican support. Trump's average support appears robust in many midwestern and southern states, which are known for their conservative voter base. However, in more liberal-leaning states such as California and New York, the average support is lower, reflecting these states' tendency to lean Democratic.


# Model

The goal of our modeling strategy is twofold. Firstly, we aim to accurately predict the support percentage (PCT) for Harris and Trump based on relevant poll data and key influencing factors. Secondly, we seek to evaluate the efficacy of different modeling approaches—from simple linear regression (SLR) to multiple linear regression (MLR) and Bayesian hierarchical models—to understand their predictive capabilities and assess the underlying relationships between variables. By comparing these models, we can determine which approach provides the most robust and reliable predictions, while considering the variability and potential uncertainty in the data.

## Model set-up

The Bayesian model is implemented in R [@citeR] using the rstanarm package as described by @rstanarm. The model is run with the following specifications:

- Formula: $\text{pct} \sim \text{pollscore} + \text{days taken from election} + \text{sample size} + (1 | \text{methodology}) + (1 | \text{state})$

- Priors: Normal(0, 2.5) for all coefficients and intercept, Exponential(1) for $\sigma$

- Settings: Seed = 123, Cores = 4, Adapt delta = 0.95

We run the model in R [@citeR] using the `rstanarm` package of @rstanarm. We use the default priors from `rstanarm`.
us

## Basic Model
```{r}
#| label: fig-model-pct-pollscore
#| echo: false
#| warning: false
just_harris_data = Harris_data |> na.omit()
lm_model1 = lm(pct ~ pollscore, data = just_harris_data)
predictions = predict(lm_model1, just_harris_data)
# summary(lm_model1)

ggplot(just_harris_data, aes(x = pct, y = predictions)) +
  geom_point(color = 'blue') +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Actual vs Predicted Values",
       x = "Actual pct",
       y = "Predicted pct") +
  theme_minimal()
```

@fig-model-pct-pollscore in this SLR model, the response variable is pct and the only one predictor is the pollscore. The primary concern lies in the evident dispersion of data points, which are widely spread and do not cluster closely around the line of perfect prediction (the dashed red line). This suggests that while pollscore may have some predictive capability, it does not adequately explain the variability in pct. The observed inconsistencies between actual and predicted values indicate that the relationship between pct and pollscore is likely not sufficiently captured by a linear model with just one predictor.

```{r}
#| echo: false
#| warning: false
just_harris_data = Harris_data |> na.omit()
model_MLR = lm(pct ~ pollscore + days_taken_from_election + methodology + sample_size + state, data = just_harris_data)
# summary(model_MLR)
just_harris_data <- just_harris_data |> mutate(fitted_value = predict(model_MLR),  num_harris = round((pct / 100) * sample_size, 0))

predictions = predict(model_MLR, just_harris_data)

ggplot(just_harris_data, aes(x = end_date)) +
  geom_point(aes(y = pct), color = "black") +
  geom_line(aes(y = fitted_value), color = "blue", linetype = "dotted") +
  facet_wrap(vars(methodology)) +
  theme_classic() +
  labs(y = "Harris percent", x = "Date", title = "Linear Model: pct ~ end_date + pollster")
```

```{r}
#| label: fig-model-MLR
#| echo: false
#| warning: false
ggplot(just_harris_data, aes(x = pct, y = fitted_value)) +
  geom_point(color = 'blue') +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Actual vs Predicted Values",
       x = "Actual pct",
       y = "Predicted pct") +
  theme_minimal()
```

@fig-model-MLR, in this MLR model, we predicts pct using pollscore, days_taken_from_election, methodology, sample_size, and state as predictors. The overall distribution of points suggests that the MLR model has captured a substantial portion of the variance in pct. The majority of the data points align relatively well with the red dashed line, particularly within the middle range of pct values (approximately between 40 and 60). This alignment indicates that the model performs reasonably well in this range, with predicted values correlating strongly with the actual observed outcomes.

## Bayesian Model
```{r}
#| label: fig-bayesian-model
#| echo: false
#| warning: false
baye_model_data = just_harris_data
baye_model_data$pct = as.factor(baye_model_data$pct)
baye_model_data$pollscore = as.factor(baye_model_data$pollscore)
baye_model_data$days_taken_from_election = as.factor(baye_model_data$days_taken_from_election)
baye_model_data$methodology = as.factor(baye_model_data$methodology)
baye_model_data$pct <- as.numeric(baye_model_data$pct)
# Define the Bayesian model with brms
# test and train dataset

formula <- pct ~ pollscore + days_taken_from_election + sample_size + (1 | methodology) + (1 | state)

priors = normal(0, 2.5, autoscale = TRUE)

bayesian_model_1 <- stan_glmer(
  formula = formula,
  data = just_harris_data,
  family = gaussian(),
  prior = priors,
  prior_intercept = priors,
  seed = 123,
  cores = 4,
  adapt_delta = 0.95
)
pp_check(bayesian_model_1)
# summary(bayesian_model_1)
```


### Posterior Predictive Checks 
@fig-bayesian-model This plot displays the results of a posterior predictive check (PPC) for a Bayesian model using the pp_check() function. 

The observed data (y) follows a distinct, symmetrical distribution centered around a specific value, suggesting a well-defined peak with tapering on both sides. The replicated distributions (y_rep), shown as multiple thin lines, generally align with the shape of the observed distribution, indicating that the Bayesian model has captured the main characteristics of the data. However, the variability in the y_rep lines highlights the degree of uncertainty inherent in the model’s predictive capability.

The fact that the y_rep curves closely match the overall pattern of the actual y suggests that the model performs reasonably well in replicating the observed data. Minor discrepancies or deviations between the y and y_rep might imply areas where the model could be fine-tuned or adjusted to improve accuracy. Overall, this PPC indicates that the model provides a decent fit to the data, with some variability accounted for in the predictive samples.

### Train Test Validation

```{r}
#| warning: false
# Split data into training and test sets
set.seed(123)
train_indices <- sample(seq_len(nrow(just_harris_data)), size = 0.7 * nrow(just_harris_data))
train_data <- just_harris_data[train_indices, ]
test_data <- just_harris_data[-train_indices, ]

# Fit the model on the training data
bayesian_model_train <- stan_glmer(
  formula = formula,
  data = train_data,
  family = gaussian(),
  prior = priors,
  prior_intercept = priors,
  seed = 123,
  cores = 4,
  adapt_delta = 0.95
)

# Predict on the test data and check performance
predictions <- posterior_predict(bayesian_model_train, newdata = test_data)
```


```{r}
#| warning: false
ss_total <- sum((test_data$pct - mean(test_data$pct))^2)
ss_residual <- sum((test_data$pct - colMeans(predictions))^2)
r_squared <- 1 - (ss_residual / ss_total)
print(paste("R-squared:", r_squared))
# "R-squared: 0.700563202695799"
```

\begin{align}
SS_{\text{total}} = \sum_{i=1}^{n} \left( y_i - \bar{y} \right)^2\\
SS_{\text{residual}} = \sum_{i=1}^{n} \left( y_i - \hat{y}_i \right)^2\\
R^2 = 1 - \frac{SS_{\text{residual}}}{SS_{\text{total}}}\\
\text{R-squared} = 0.700563202695799
\end{align}

Where:\
\begin{itemize}
    \item \( y_i \) represents the actual values,
    \item \( \bar{y} \) is the mean of the actual values,
    \item \( \hat{y}_i \) are the predicted values,
    \item \( SS_{\text{total}} \) is the total sum of squares,
    \item \( SS_{\text{residual}} \) is the sum of squared residuals.
\end{itemize}



we could see that the R squared value equal to the 0.700563202695799 suggesting that the model captures a substantial portion of the variability in the data and demonstrates that the model performs well in explaining the variability of the pct in the test data, reflecting strong predictive capabilities.

\newpage
```{r}
#| label: fig-bayesian-model-actual-predict
#| echo: false
#| warning: false
plot(test_data$pct, colMeans(predictions), 
     xlab = "Actual PCT Values", 
     ylab = "Predicted  PCT Values", 
     main = "Predicted PCT vs Actual PCT")
abline(0, 1, col = "red")  # Add a 45-degree reference line
```

 by visual checking actual pct and predicted pct plot @fig-bayesian-model-actual-predict the points are plotted against a 45-degree line, which represents the ideal scenario where predicted values match the actual values perfectly.

\newpage
```{r}
#| label: fig-bayesian-model-residual
#| echo: false
#| warning: false
residuals <- test_data$pct - colMeans(predictions)
plot(colMeans(predictions), residuals, 
     xlab = "Predicted Values", 
     ylab = "Residuals", 
     main = "Residual Plot")
abline(h = 0, col = "red")
```

and the residual plot @fig-bayesian-model-residual are fairly centered around zero with no major trend, suggesting that the model is not heavily biased in its predictions.

In conculsion, the visual checks from the predicted vs. actual plot and the residual plot, there is no strong evidence that the Bayesian model is overfitting. It appears to generalize well to the data it was trained on without showing signs of capturing noise or irrelevant patterns. 

### Model justification

To predict the support percentage (PCT) for Harris and Trump, we employed a comprehensive modeling strategy involving Simple Linear Regression (SLR), Multiple Linear Regression (MLR), and a Bayesian hierarchical model. The SLR model provided a foundational analysis of the relationship between PCT and pollscore, highlighting its limited predictive power due to a lack of complexity. The MLR model improved on this by incorporating additional predictors such as days taken from the election, sample size, methodology, and state, enhancing its predictive accuracy and capturing interactions among variables. The Bayesian hierarchical model further refined our approach, incorporating prior knowledge and accounting for variability at group levels (e.g., methodology and state) through random intercepts. This model provided robust uncertainty quantification through credible intervals and demonstrated strong predictive performance with an R-squared value around 0.70, indicating it effectively explained data variability. By leveraging the strengths of these models, particularly the Bayesian approach’s interpretability and robustness, we achieved reliable predictions and a deeper understanding of the factors influencing support percentages.


# Result
```{r}
#| label: fig-bayesian-model-CI
#| echo: false
#| warning: false
# Plot random effects
plot(bayesian_model_1, pars = "(Intercept)", prob = 0.95)
```


@fig-bayesian-model-CI This plot displays the estimated posterior distribution for the intercept parameter in the Bayesian model. The intercept represents the baseline level of support, expressed as a percentage, for Kamala Harris in the U.S. election data used in the analysis. The plot features a point estimate (depicted by the central dot) that indicates the mean or median of the posterior distribution for the intercept, and a horizontal line showing the 95% credible interval, which signifies the range within which the true value of the intercept is likely to fall with 95% probability.

The range of this credible interval spans from approximately 50 to 54 percent, suggesting that the model estimates the baseline level of support for Kamala Harris to be around this range. The relatively narrow width of the interval implies a certain degree of confidence in the estimate, indicating that the data used in the model provided a clear signal for the intercept's value.

In conclusion, the estimated intercept, which represents the baseline percentage of support for Kamala Harris in the U.S. election data analyzed, is centered around 52%, with a credible interval spanning approximately from 50% to 54%. This suggests that, according to the Bayesian model, the underlying support for Kamala Harris.


```{r}
#| echo: false
#| warning: false

predictions2 <- posterior_predict(bayesian_model_1, newdata = just_harris_data)

predicted_means <- colMeans(predictions2)
predicted_intervals <- apply(predictions2, 2, quantile, probs = c(0.025, 0.975))

# Combine the results with the actual data
result_summary <- data.frame(
  Actual_PCT = just_harris_data$pct,
  Predicted_PCT = predicted_means,
  Lower_CI = predicted_intervals[1, ],
  Upper_CI = predicted_intervals[2, ]
)

ggplot(result_summary, aes(x = Actual_PCT, y = Predicted_PCT)) +
  geom_point() +
  geom_errorbar(aes(ymin = Lower_CI, ymax = Upper_CI), width = 0.2, color = "blue") +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  labs(
    title = "U.S. Election Prediction Results",
    x = "Actual PCT",
    y = "Predicted PCT"
  )

print(head(result_summary))

```



# Discussion

## First discussion point {#sec-first-point}

If my paper were 10 pages, then should be be at least 2.5 pages. The discussion is a chance to show off what you know and what you learnt from all this. 

## Second discussion point

Please don't use these as sub-heading labels - change them to be what your point actually is.

## Third discussion point

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

\appendix

# Appendix {-}


# Additional data details

# Model details {#sec-model-details}

## Posterior predictive check

In @fig-ppcheckandposteriorvsprior-1 we implement a posterior predictive check. This shows...

In @fig-ppcheckandposteriorvsprior-2 we compare the posterior with the prior. This shows... 

## Diagnostics

@fig-stanareyouokay-1 is a trace plot. It shows... This suggests...

@fig-stanareyouokay-2 is a Rhat plot. It shows... This suggests...

# FiveThirtyEight Licenses
[FiveThirtyEight's data sets](https://github.com/fivethirtyeight/data/tree/master/polls) are used and modified by us under the [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/).

\newpage


# References


