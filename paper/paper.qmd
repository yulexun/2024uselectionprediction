---
title: "Forecasting the 2024 U.S. Presidential Election through Poll Aggregation and Adjustments for Poll Quality"
subtitle: "A Hybrid Model Predicts a Narrow Lead for Kamala Harris in Swing States as Election Day Approaches"
author: 
  - Colin Sihan Yang
  - Lexun Yu
  - Siddharth Gowda
thanks: "Code and data are available at: [https://github.com/yulexun/uselection](https://github.com/yulexun/uselection)."
date: today
date-format: long
abstract: "This paper develops a hybrid model to forecast the 2024 U.S. presidential election by combining poll aggregation techniques with additional variables, including pollster reliability, sample size, timing relative to the election, geographic region, and polling methodology. By integrating these factors, our model accounts for variations in poll quality and regional differences in voter sentiment, providing a more stable and accurate prediction compared to traditional poll aggregation alone. Our results show that Kamala Harris holds a slight lead over Donald Trump in most swing states, with support levels stabilizing closer to election day. This approach underscores the importance of nuanced forecasting models in election analysis, revealing how poll quality and timing can significantly influence support predictions. Ultimately, this model offers a comprehensive tool for understanding the dynamics of voter sentiment and improving the accuracy of election predictions."
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(here)
library(tibble)
library(knitr)
library(arrow)
library(rstanarm)
library(brms)
library(dplyr)
library(lubridate)
library(gridExtra)
library(Matrix)
library(rstan)

light_blue <- "#ADD8E6"
dark_blue <- "#6688CC"
light_red <- "#FF7777"
dark_red <- "#B22222"
purple <- "#9370DB"

cleaned_data <- read_parquet(here("data/02-analysis_data/cleaned_data.parquet"))
```


# Introduction {#sec-intro}
Election result forecasting has become an essential tool for analysts in political science and the public to predict the outcome of democratic process, such as the presidential election in the United States. Traditionally, individual polls have been used as a snapshot of voter sentiment, but they only reflect temporary changes in the performance of contestants, instead of a precise estimation of the election result. As discussed by @pasek and @blumenthal, the aggregation of multiple polls, or "poll-of-polls," has become a popular technique to reduce individual survey errors and provide more accurate election forecasts. However, conventional aggregation models often overlook dynamic election factors, including the credibility and quality variations among poll sources.

We build a hybrid election forecasting model following the strategies mentioned by @pasek. As @pasek described in their article, aggregation involves determining which surveys are worth including, as well as selecting, combining and averaging results from multiple polls to reduce individual biases and errors. Our model incorporates this approach, filtering polls from @fivethirtyeight based on a numeric grade that indicates each pollster's reliability.

The objective of this study is to predict the support percentages for the primary candidates, Kamala Harris and Donald Trump, using this hybrid model. We incorporate aggregation by filtering the polls on @fivethirtyeight by numeric grade that indicates pollster’s reliability. We also quantify other polling attributes, such as sample size, poll reliability scores, and polling methodology, which serve as predictors. Our results show that this model provides a more stable and accurate forecast than traditional poll aggregation alone.

The remainder of this paper is structured as follows: @sec-data provides an overview and exploration of the data. @sec-model provides the modeling approach, including simple and multiple linear regression models and a Bayesian hierarchical model. We then present our results in @sec-result and discuss the implications, limitations, and future research directions in @sec-discussion.

The data gathering and analysis is done in R [@citeR] with the following packages: knitr [@knitr], tidyverse [@tidyverse], ggplot2 [@ggplot2], dplyr [@dplyr], arrow [@arrow], here [@here], gridExtra [@gridextra], Matrix [@Matrix], Rstan [@Rstan] and lubridate [@lubridate].

## Estimand

The estimand for this research paper is the predicted support percentages for Kamala Harris and Donald Trump. The prediction is based on quantifying various polling factors, including sample size, poll scores, and transparency scores, which are used as predictors.

# Data {#sec-data}

## Measurement

The dataset we obtained from @fivethirtyeight is accumulated from multiple polls and surveys. According to FiveThirtyEight, they aggregate polling data conducted by other firms and organizations that meets their methodological and ethical standards [@morris_trump_2024]. The dataset contains a list of questions in polls and surveys and their results. When a new poll is conducted, the poll is appended to the dataset. FiveThirtyEight assign the poll a poll_id, and each question is assigned a question_id. 

In each of the polls recorded by FiveThirtyEight, all options of each question are recorded in candidate_names, while the proportion of respondents choosing that option is recorded in pct as a percentage. The pct is our primary response variable. 

There are limitations in these measurements. The differences in sampling methods, wording in survey questions and systematic biases are all reflected in the outcome [@radcliffe_538s_2023]. Thus, in the dataset, FiveThirtyEight includes other variables, such as pollscore and sample_size for their prediction model in addition to the polling results for transparency and accuracy. As an example, pollscore indicates the pollster’s reliability rating, a low pollscore indicates a higher reliability rating [@silver_polls_2008].

In this dataset, each row represents a polling question that records the variables of interest. Each entry allows us to explore the real-world relationships between polling factors and the support percentage (`pct`) for the candidates Kamala Harris and Donald Trump. This dataset enables an analysis of how various polling characteristics influence the reported support levels for the candidates we are focused.

These variables combined allow researchers to reliably analyze and predict the 2024 US election result over time.

## Data Exploration

The raw data from FiveThirtyEight contains 52 columns, all of the column headers are displayed below: 
```{r}
#| echo: false
#| warning: false
raw_data <- read_csv(here("data/01-raw_data/president_polls.csv"))

# Get the column names of your data
column_names <- colnames(raw_data)

# Reshape the column names into a matrix with, for example, 4 columns
num_cols <- 3
column_matrix <- matrix(column_names, ncol = num_cols, byrow = TRUE)

# Convert the matrix to a data frame for kable
column_df <- as.data.frame(column_matrix)

# Display in multiple columns with kable
kable(column_df, format = "latex", booktabs = TRUE, col.names = NULL)
```

These columns can be categorized into three types, response variable, numeric predictors and categorical predictors.

The response variable is:

- pct: The percentage of support or vote share that each candidate (Kamala Harris or Donald Trump) received in the poll.

Example of numeric predictors are:

- sample_size: The total number of respondents in each poll.
- pollscore: A numeric score representing the error and bias of the pollster. Negative numbers are better.
- numeric_grade: A numeric grade assigned to each pollster, reflecting pollster quality or reliability.

Example of categorical predictors are:

- state: The U.S. state in which the poll was conducted or targeted.
- methodology: The method used to conduct the poll (e.g., online, phone, in-person).
- party: The political party of the candidate (e.g., DEM for Democrat, REP for Republican).
- candidate_name: The name of the candidate, either Kamala Harris or Donald Trump.
- pollster: The name of the polling organization that conducted the poll.
- stage: The stage of the election (e.g., "general").



<!-- ## Data Exploration

```{r}
#| label: fig-pairs
#| fig-cap: "Pairs Plot of Key Numeric Variables Related to Percentage Support (pct)"
#| echo: false
#| warning: false
# numeric_data <- cleaned_data[sapply(cleaned_data, is.numeric)]
numeric_data = cleaned_data |> select(pct, sample_size, pollscore, days_taken_from_election)

# Create the pairs plot
pairs(numeric_data)
```

In @fig-pairs, the pairs plot displays scatter plots of four numeric variables (`pct`, `sample_size`, `pollscore`, and `days_taken_from_election`) to visualize their relationships. The data shows clustering, particularly in `pct` versus `sample_size`, suggesting potential heteroscedasticity. The `sample_size` variable is skewed towards lower values, while `pollscore` and `days_taken_from_election` have a more even spread, though `pollscore` shows central clustering. No strong linear relationships are immediately apparent between the variables, indicating that correlations are likely weak.  -->


## Clean Data
The data cleaning process involves several steps to ensure the quality and relevance of the polling data. First, we filter the dataset to retain only poll results with a numeric grade of 2.7 or higher, indicating that the polls are considered reliable. Next, we address missing values in the state attribute: polls with NA in the state column are considered national polls.

We then create a new attribute, days_taken_from_election, which represents the time gap between the poll's start date and the actual U.S. election date. Additionally, we filter the dataset to include only polls conducted after July 21, 2024, the date when Kamala Harris declared her candidacy. Finally, we remove any remaining rows that contain missing values to ensure a clean dataset.

### States included in anaylsis

After the data cleaning process, 21 states had no polling data. A table showing the number of polls for each state, including those without any polls, is provided in @tbl-state-polls-count.

This absence of polling data is not a significant concern due to the structure of the United States Electoral College (explained in detail in the Appendix). The states lacking polling data have consistently followed historical voting patterns, so predicting the winning candidate in those states is unnecessary.

## Cleaned data

The first 6 rows of the dataset is displayed in @tbl-cleaned-data.

```{r}
#| label: tbl-cleaned-data
#| tbl-cap: Sample of cleaned US election data 
#| echo: false
#| warning: false

cleaned_data |>
  select(pct, sample_size, pollscore, days_taken_from_election, state, methodology, candidate_name) |>
  head(6) |>
  kable(
    col.names = c("pct", "Sample Size", "pollscore", "Days taken from election","state", "methodology", "Candidate name"),
    booktabs = TRUE
    ) 
```

### Basic Statistics Summary for Cleaned Data

```{r}
#| label: fig-avg-pct-state
#| fig-cap: Historical State Voting Trends Are Maintined in 2024
#| echo: false
#| warning: false
#| 
cleaned_data <- cleaned_data %>% mutate(candidate_name =
  if_else(!(candidate_name %in% c("Kamala Harris", "Donald Trump")), "Other", candidate_name))

cleaned_data %>%
  filter(candidate_name %in% c("Kamala Harris", "Donald Trump")) %>%
  group_by(state, candidate_name) %>%
  summarise(avg_pct = mean(pct, na.rm = TRUE), .groups = "drop") %>%
  ggplot(mapping = aes(x = avg_pct, y = state, fill = candidate_name)) + 
  geom_col(position = "stack") +
  theme_minimal() +
  labs(
    x = "Average Polled Support (%)", 
    y = "State",
    fill = "Candidate"
  ) +
  scale_fill_manual(values = c("Kamala Harris" = light_blue, "Donald Trump" = light_red)) +
  theme_minimal() +
  theme(legend.position = "bottom") + 
  theme(axis.text.y = element_text(size = 8))

```

In @fig-avg-pct-state, historically Democratic are polling for Kamala and historically Republican states are polling for Trump. Similarly, historically swing states also appear to be close, for instance Michigan (46.9% Trump, 47.5% Harris), Nevada (47.7% Trump, 47.9% Harris), and Pennsylvania (46.9% Trump, 48.2% Harris) all are close to an even split.

```{r}
#| label: fig-avg-pct-methodology
#| fig-cap: Polls with more Methodologies have Less Variability.
#| echo: false
#| warning: false
#| 
cleaned_data <- cleaned_data %>% mutate(candidate_name =
  if_else(!(candidate_name %in% c("Kamala Harris", "Donald Trump")), "Other", candidate_name))

cleaned_data %>%
  filter(candidate_name %in% c("Kamala Harris", "Donald Trump")) %>%
  filter(!is.na(methodology)) %>% 
  ggplot(mapping = aes(x = methodology, y = pct)) + 
  geom_boxplot(width = 0.6, position = position_dodge(width = 0.8), outlier.shape = NA) +
  coord_flip() +
  theme_minimal() +
  labs(
    x = "Methodology",
    y = "Polled Support (%)",
    fill = "Candidate"
  ) +
  scale_fill_manual(values = c("Kamala Harris" = light_blue, "Donald Trump" = light_red)) +
  theme(legend.position = "bottom") + 
  theme(axis.text.y = element_text(size = 8))

```

@fig-avg-pct-methodology shows that polls utilizing multiple communication methods to reach voters tend to have lower interquartile ranges (IQRs) in their boxplots compared to those that rely on only one or two communication methods.

### Relationship Between Variables

```{r}
#| label: fig-pollscore-samplesize
#| fig-cap: More Reliable Pollsters Have Larger Sample Sizes in their Polls
#| echo: false
#| warning: false

# removing outliers and na's
graph_data <- cleaned_data %>% 
  filter(!is.na(pollscore) & !is.na(sample_size)) 

outlier_top <- (quantile(graph_data$sample_size, 0.75) + 
                  IQR(graph_data$sample_size)*1.5)

outlier_bottom <- (quantile(graph_data$sample_size, 0.25) - 
                  IQR(graph_data$sample_size)*1.5)

graph_data <- cleaned_data %>%
  filter(sample_size < outlier_top & sample_size > outlier_bottom)

graph_data %>% filter(!is.na(pollscore) & !is.na(sample_size)) %>% 
  ggplot(aes(x = pollscore, y = sample_size)) +
  geom_point(alpha = 0.33) + geom_smooth(method = "lm", se = FALSE, color = light_red) +
  labs(x = "Poll Score", y = "Poll Sample Size")

# this model is really here just for explain, this realtionship should not
# be treated as lienar
# cor(graph_data$sample_size, graph_data$pollscore)
# lm_score_sample <- lm(sample_size ~ pollscore, data = graph_data)
# summary(lm_score_sample)
```

@fig-pollscore-samplesize illustrates a weak positive correlation between a pollster's pollscore and the sample size of their poll. The figure also shows that most polls have a sample size around 800 to 1200 participants. It is also important to note that a few polls with exceptionally large sample sizes were excluded from the graph due to their status as clear outliers.

```{r}
#| label: fig-pollsamplesize-canidate-pct
#| fig-cap: The Sample Size of a Poll does not impact a Candidate's Voting Percentage
#| echo: false
#| warning: false

cleaned_data %>% 
  filter(!is.na(sample_size) & !is.na(pct)) %>% 
  ggplot(aes(x = sample_size, y = pct)) +
  geom_point(alpha = 0.4) + 
  geom_smooth(method = "lm", se = FALSE, color = light_red) +
  labs(x = "Sample Size of the Poll",
       y = "Candidate Percentage") + facet_wrap(~candidate_name)

```

Based on @fig-pollsamplesize-canidate-pct, there does not seem to be a relationship between the sample size of a poll the percentage of a candidate.

```{r}
#| label: fig-pollscore-candidate-pct
#| fig-cap: More Reliable Pollsters Score Higher For Trump
#| echo: false
#| warning: false

cleaned_data %>% 
  filter(!is.na(pollscore) & !is.na(pct)) %>% 
  ggplot(aes(x = pollscore, y = pct)) +
  geom_point(alpha = 0.4) + 
  geom_smooth(method = "lm", se = FALSE, color = light_red) +
  labs(x = "Pollscore",
       y = "Candidate Percentage") + facet_wrap(~candidate_name)


```

@fig-pollscore-candidate-pct depicts the relationship between a pollster's poll score and a candidate's percentage. For Donald Trump, a negative correlation is observed, indicating that pollsters with lower poll scores tend to assign him a higher percentage of support. Conversely, Kamala Harris shows the opposite trend: as the poll score decreases, her percentage tends to rise. This suggests that more reliable polls, characterized by lower poll scores, report higher support for Trump compared to less reliable pollsters.

```{r}
#| label: fig-polldate-canidate-pct
#| fig-cap: Both Canidate Percentages Are Gaining Support Closer to the Election
#| echo: false
#| warning: false

cleaned_data %>% 
  filter(!is.na(days_taken_from_election) & !is.na(pct)) %>% 
  ggplot(aes(x = days_taken_from_election, y = pct)) +
  geom_point(alpha = 0.4) + 
  geom_smooth(method = "lm", se = FALSE, color = light_red) +
  labs(x = "Timing of Polls Relative to Election Day (Days Prior to the Election)",
       y = "Candidate Percentage") + facet_wrap(~candidate_name)

```

Based on @fig-polldate-canidate-pct, Trump and Harris are getting more votes in polls that are done closer to the election. This is a result of non-major candidate support rapidly decreasing. Specifically, Trump support is increasing at a faster rate than Harris. However, the polls in general show a slight lead for Harris throughout the last 100 days. 

The rapid decline in third party support could be due to Robert F. Kennedy dropping out of the race (this sentences should probably be in results or discussion section).

# Model {#sec-model}

The goal of our modeling strategy is twofold. Firstly, we aim to accurately predict the support percentage (PCT) for Harris and Trump based on relevant poll data and key influencing factors. Secondly, we seek to evaluate the efficacy of different modeling approaches—from simple linear regression (SLR) to multiple linear regression (MLR) and Bayesian hierarchical models—to understand their predictive capabilities and assess the underlying relationships between variables. By comparing these models, we can determine which approach provides the most robust and reliable predictions, while considering the variability and potential uncertainty in the data.

## Model set-up {#sec-modelsetup}

The Bayesian model is implemented in R [@citeR] using the rstanarm package as described by @rstanarm. The model is run with the following specifications:

- Formula: $\text{pct} \sim \text{pollscore} + \text{days taken from election} + \text{sample size} + (1 | \text{methodology}) + (1 | \text{state})$

- Priors: Normal(0, 2.5) for all coefficients and intercept, Exponential(1) for $\sigma$

- Settings: Seed = 123, Cores = 4, Adapt delta = 0.95

We run the model in R [@citeR] using the `rstanarm` package of @rstanarm. We use the default priors from `rstanarm`.
us

## Basic SLR Model
```{r}
#| label: fig-model-pct-pollscore
#| fig-cap: "Comparison of Actual vs Predicted Percentage Support for Kamala Harris (Single Predictor: Pollscore)"
#| echo: false
#| warning: false

Harris_data = cleaned_data[cleaned_data$candidate_name == "Kamala Harris", ]
just_harris_data = Harris_data |> na.omit()
lm_model1 = lm(pct ~ pollscore, data = just_harris_data)
predictions = predict(lm_model1, just_harris_data)
# summary(lm_model1)

ggplot(just_harris_data, aes(x = pct, y = predictions)) +
  geom_point(color = 'blue') +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Actual vs Predicted Values",
       x = "Actual pct",
       y = "Predicted pct") +
  theme_minimal()
```


We first set up a SLR model for predicting pct based on pollscore. The model is: 


$\hat{\text{pct}} = \beta_0 + \beta_1 \times \text{pollscore}$


In this SLR model, the response variable is pct and the only one predictor is the pollscore. @fig-model-pct-pollscore visualizes the relationship between the actual and predicted values of percentage support (pct) for Kamala Harris, based on a simple linear regression model with pollscore as the sole predictor. Blue points represent individual comparisons between actual and predicted values. The red dashed line represents the line of perfect prediction, where actual values would equal predicted values.

The primary concern lies in the evident dispersion of data points, which are widely spread and do not cluster closely around the line of perfect prediction (the dashed red line). This suggests that while pollscore may have some predictive capability, it does not adequately explain the variability in pct. The observed inconsistencies between actual and predicted values indicate that the relationship between pct and pollscore is likely not sufficiently captured by a linear model with just one predictor.

```{r}
#| label: fig-model-pct-sample_size
#| fig-cap: "Actual vs. Predicted Values for pct Using sample_size as Predictor"
#| echo: false
#| warning: false
lm_model1 = lm(pct ~ sample_size, data = just_harris_data)
predictions = predict(lm_model1, just_harris_data)
# summary(lm_model1)

ggplot(just_harris_data, aes(x = pct, y = predictions)) +
  geom_point(color = 'blue') +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Actual vs Predicted Values",
       x = "Actual pct",
       y = "Predicted pct") +
  theme_minimal()

```

In @fig-model-pct-sample_size, the distribution of the points suggests that the model might not capture much variability in pct when sample_size is the only predictor. This indicate that sample_size has limited impact on pct.

## MLR model
```{r}
#| label: fig-model-pct-pollscore-etc
#| echo: false
#| warning: false
just_harris_data = Harris_data |> na.omit()
model_MLR = lm(pct ~ pollscore + days_taken_from_election + methodology + sample_size + state, data = just_harris_data)
# summary(model_MLR)
just_harris_data <- just_harris_data |> mutate(fitted_value = predict(model_MLR),  num_harris = round((pct / 100) * sample_size, 0))

predictions = predict(model_MLR, just_harris_data)


```

```{r}
#| label: fig-model-MLR
#| fig-cap: "Actual vs Predicted Percentage Support for Kamala Harris (Multiple Predictors)"
#| echo: false
#| warning: false
ggplot(just_harris_data, aes(x = pct, y = fitted_value)) +
  geom_point(color = 'blue') +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Actual vs Predicted Values",
       x = "Actual pct",
       y = "Predicted pct") +
  theme_minimal()
```

Next, we predicts pct using pollscore, days teken from election, methodology, sample size, and state as predictors. The model is: 

$\hat{\text{pct}} = \beta_0 + \beta_1 \cdot \text{pollscore} + \beta_2 \cdot \text{days\_taken\_from\_election} + \beta_3 \cdot \text{methodology} + \beta_4 \cdot \text{sample\_size} + \beta_5 \cdot \text{state}$

@fig-model-MLR compares actual pct values with the predicted pct values from the MLR model. Blue points represent individual comparisons between actual and predicted values. The red dashed line represents the line of perfect prediction, where actual values would equal predicted values.

The overall distribution of points suggests that the MLR model has captured a substantial portion of the variance in pct. The majority of the data points align relatively well with the red dashed line, particularly within the middle range of pct values (approximately between 40 and 60). This alignment indicates that the model performs reasonably well in this range, with predicted values correlating strongly with the actual observed outcomes.

## Bayesian Model

Next, we set up a bayesian model as the following in accordance to @sec-modelsetup: 

\begin{align*}
\text{pct}_i &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_1 \cdot \text{pollscore}_i + \beta_2 \cdot \text{days\_taken\_from\_election}_i + \beta_3 \cdot \text{sample\_size}_i \\
+ u_{\text{methodology}[i]} + u_{\text{state}[i]} \\
\alpha &\sim \text{Normal}(0, 2.5) \\
\beta_j &\sim \text{Normal}(0, 2.5), \quad j = 1, 2, 3 \\
u_{\text{methodology}} &\sim \text{Normal}(0, \sigma_{\text{methodology}}) \\
u_{\text{state}} &\sim \text{Normal}(0, \sigma_{\text{state}})
\end{align*}

This hierarchical Bayesian model is designed to capture both the fixed effects of predictors and the random effects of grouping factors. The response variable is pct (percentage of support), predictors are pollscore, days taken from election, and sample size. A normal prior with mean 0 and standard deviation 2.5 is set for all coefficients and the intercept, scaled automatically.


```{r}
#| label: fig-bayesian-model
#| fig-cap: "Posterior Predictive Check for the Bayesian Model Predicting pct"
#| echo: false
#| warning: false
bayesian_model_1 = readRDS(here("models/bayesian_model_1.rds"))

pp_check(bayesian_model_1)
```


### Posterior Predictive Checks 
@fig-bayesian-model displays the results of a posterior predictive check (PPC) for a Bayesian model using the pp_check() function. 

The observed data (y) follows a distinct, symmetrical distribution centered around a specific value, suggesting a well-defined peak with tapering on both sides. The replicated distributions (y_rep), shown as multiple thin lines, generally align with the shape of the observed distribution, indicating that the Bayesian model has captured the main characteristics of the data. However, the variability in the y_rep lines highlights the degree of uncertainty inherent in the model’s predictive capability.

The fact that the y_rep curves closely match the overall pattern of the actual y suggests that the model performs reasonably well in replicating the observed data. Minor discrepancies or deviations between the y and y_rep might imply areas where the model could be fine-tuned or adjusted to improve accuracy. Overall, this PPC indicates that the model provides a decent fit to the data, with some variability accounted for in the predictive samples.

### Train Test Validation
we implemented a train-test split validation approach to evaluate the performance of a Bayesian hierarchical model predicting percentage support (pct) for Kamala Harris. The dataset was divided into training (70%) and test (30%) sets to fit and validate the model, respectively. We used the stan_glmer function to fit the model on the training data, with predictors including pollscore, days_taken_from_election, sample_size, and random intercepts for methodology and state. After training, we generated predictions on the test set and calculated an R squared value to quantify the model's predictive accuracy. 
```{r}
#| echo: false
#| warning: false
# Split data into training and test sets
set.seed(123)
# train_indices <- sample(seq_len(nrow(just_harris_data)), size = 0.7 * nrow(just_harris_data))
# train_data <- just_harris_data[train_indices, ]
# test_data <- just_harris_data[-train_indices, ]
train_data = read_parquet(here("data/02-analysis_data/train_data.parquet"))
test_data = read_parquet(here("data/02-analysis_data/test_data.parquet"))

# Fit the model on the training data
bayesian_model_train = readRDS(here("models/bayesian_model_train.rds"))
# bayesian_model_train <- stan_glmer(
#   formula = formula,
#   data = train_data,
#   family = gaussian(),
#   prior = priors,
#   prior_intercept = priors,
#   seed = 123,
#   cores = 4,
#   adapt_delta = 0.95
# )

# Predict on the test data and check performance
predictions <- posterior_predict(bayesian_model_train, newdata = test_data)
```

\begin{align}
SS_{\text{total}} = \sum_{i=1}^{n} \left( y_i - \bar{y} \right)^2\\
SS_{\text{residual}} = \sum_{i=1}^{n} \left( y_i - \hat{y}_i \right)^2\\
R^2 = 1 - \frac{SS_{\text{residual}}}{SS_{\text{total}}}\\
\text{R-squared} = 0.700563202695799
\end{align}

```{r}
#| warning: false
#| echo: false
ss_total <- sum((test_data$pct - mean(test_data$pct))^2)
ss_residual <- sum((test_data$pct - colMeans(predictions))^2)
r_squared <- 1 - (ss_residual / ss_total)
# "R-squared: 0.700563202695799"
```

Where:\
\begin{itemize}
    \item \( y_i \) represents the actual values,
    \item \( \bar{y} \) is the mean of the actual values,
    \item \( \hat{y}_i \) are the predicted values,
    \item \( SS_{\text{total}} \) is the total sum of squares,
    \item \( SS_{\text{residual}} \) is the sum of squared residuals.
\end{itemize}

We could see that the R squared value equal to the 0.700563202695799 suggesting that the model captures a substantial portion of the variability in the data and demonstrates that the model performs well in explaining the variability of the pct in the test data, reflecting strong predictive capabilities.

```{r}
#| label: fig-bayesian-model-actual-predict
#| fig-cap: "Predicted vs Actual Percentage Support (pct) for Kamala Harris in the Bayesian Model"
#| echo: false
#| warning: false
plot(test_data$pct, colMeans(predictions), 
     xlab = "Actual PCT Values", 
     ylab = "Predicted  PCT Values", 
     main = "Predicted PCT vs Actual PCT")
abline(0, 1, col = "red")  # Add a 45-degree reference line
```

 By visual checking actual pct and predicted pct plot in @fig-bayesian-model-actual-predict, the points are plotted against a 45-degree line, which represents the ideal scenario where predicted values match the actual values perfectly.

```{r}
#| label: fig-bayesian-model-residual
#| fig-cap: "Residual Plot for the Bayesian Hierarchical Model Predictions"
#| echo: false
#| warning: false
residuals <- test_data$pct - colMeans(predictions)
plot(colMeans(predictions), residuals, 
     xlab = "Predicted Values", 
     ylab = "Residuals", 
     main = "Residual Plot")
abline(h = 0, col = "red")
```

In addition, the residual plot @fig-bayesian-model-residual are fairly centered around zero with no major trend, suggesting that the model is not heavily biased in its predictions.

In conculsion, the visual checks from the predicted vs. actual plot and the residual plot, there is no strong evidence that the Bayesian model is overfitting. It appears to generalize well to the data it was trained on without showing signs of capturing noise or irrelevant patterns. 

### Model justification

To predict the support percentage (PCT) for Harris and Trump, we employed a comprehensive modeling strategy involving Simple Linear Regression (SLR), Multiple Linear Regression (MLR), and a Bayesian hierarchical model. The SLR model provided a foundational analysis of the relationship between PCT and pollscore, highlighting its limited predictive power due to a lack of complexity. The MLR model improved on this by incorporating additional predictors such as days taken from the election, sample size, methodology, and state, enhancing its predictive accuracy and capturing interactions among variables. The Bayesian hierarchical model further refined our approach, incorporating prior knowledge and accounting for variability at group levels (e.g., methodology and state) through random intercepts. This model provided robust uncertainty quantification through credible intervals and demonstrated strong predictive performance with an R-squared value around 0.70, indicating it effectively explained data variability. By leveraging the strengths of these models, particularly the Bayesian approach’s interpretability and robustness, we achieved reliable predictions and a deeper understanding of the factors influencing support percentages.


# Result {#sec-result}
```{r}
#| label: fig-bayesian-model-CI
#| fig-cap: "Estimated Posterior Distribution for the Intercept Parameter in the Bayesian Model"
#| echo: false
#| warning: false
# Plot random effects
plot(bayesian_model_1, pars = "(Intercept)", prob = 0.95)
```


@fig-bayesian-model-CI displays the estimated posterior distribution for the intercept parameter in the Bayesian model. The intercept represents the baseline level of support, expressed as a percentage, for Kamala Harris in the U.S. election data used in the analysis. The plot features a point estimate (depicted by the central dot) that indicates the mean or median of the posterior distribution for the intercept, and a horizontal line showing the 95% credible interval, which signifies the range within which the true value of the intercept is likely to fall with 95% probability.

The range of this credible interval spans from approximately 50 to 54 percent, suggesting that the model estimates the baseline level of support for Kamala Harris to be around this range. The relatively narrow width of the interval implies a certain degree of confidence in the estimate, indicating that the data used in the model provided a clear signal for the intercept's value.

In conclusion, the estimated intercept, which represents the baseline percentage of support for Kamala Harris in the U.S. election data analyzed, is centered around 52%, with a credible interval spanning approximately from 50% to 54%. This suggests that, according to the Bayesian model, the underlying support for Kamala Harris.


```{r}
#| label: tbl-bayesian-model-result-summary
#| tbl-cap: "Bayesian Model Result Summary"
#| echo: false
#| warning: false

predictions2 = posterior_predict(bayesian_model_1, newdata = just_harris_data)

predicted_means = colMeans(predictions2)
predicted_intervals = apply(predictions2, 2, quantile, probs = c(0.025, 0.975))

# Combine the results with the actual data
result_summary = data.frame(
  Actual_PCT = just_harris_data$pct,
  Predicted_PCT = predicted_means,
  Lower_CI = predicted_intervals[1, ],
  Upper_CI = predicted_intervals[2, ]
)

kable(head(result_summary, 10))
```

```{r}
#| label: fig-bayesian-model-result-CI
#| fig-cap: "U.S. Election Prediction Results"
#| echo: false
#| warning: false
ggplot(result_summary, aes(x = Actual_PCT, y = Predicted_PCT)) +
  geom_point() +
  geom_errorbar(aes(ymin = Lower_CI, ymax = Upper_CI), width = 0.2, color = "blue") +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  labs(
    title = "U.S. Election Prediction Results",
    x = "Actual PCT",
    y = "Predicted PCT"
  )
```

@fig-bayesian-model-result-CI shows that he model's predictive performance for Harris's election data appears generally robust, demonstrating a strong alignment between actual and predicted vote percentages. The majority of the data points cluster around the line, indicating reliable predictions, some variance is evident, particularly in the lower and higher actual percentage ranges. This suggests that while the model performs well on average, it may have larger uncertainty in its predictions at the extremes. @tbl-bayesian-model-result-summary also highlights that most predicted values align closely with actual values, supporting the model’s robustness in capturing the overall trend of election outcomes.


# Discussion {#sec-discussion}

## Summary of Findings

In this paper, we developed a hybrid election forecasting model to predict the 2024 U.S. presidential election results. Our model provides a stable and accurate forecast compared to traditional methods by combining poll-of-polls aggregation with additional factors such as poll score, sample size, days from the election, state, and methodology. 

Our approach in models applies simple linear regression, multiple linear regression and Bayesian hierarchical modeling that includes both fixed and random effects, capturing variations in poll reliability. The results shows that a model accounting for poll quality, timing, and methodology can produce forecasts that are more resilient to biases found in individual polls.

## Insights on Polling and Election Dynamics

One of the main takeaways from this study is the impact of poll quality on election forecasts. We weight and filtered the polls by their reliability. This approach highlights the importance of screening and adjusting for poll reliability in election forecasts, particularly in a high-stakes environment where even minor polling biases can influence public perception and campaign strategies.

## Limitations of the Model

While this model presents a more comprehensive approach, there are limitations. First, Any systematic biases inherent in those polls can carry over into the model’s predictions because we are using existing poll data. For instance, if certain demographic groups are consistently underrepresented, our model may not fully capture their impact on election outcomes. 

Second, the model only reflect current voting dynamics as it is based on historical data. This model does not account for short-term variability such as Biden's exit. This limitation is important in the final weeks before an election when small shifts in polling data can produce exaggerated effects.

## Future Directions
To enhance the reliability and robustness of this forecasting model, future work should focus on validating its performance across multiple election cycles. We includes previous year's polling dataset in this R project. By applying the model to past elections, researchers can assess its accuracy in different political contexts and electoral dynamics. This approach would show any limitations specific to certain election conditions, such as shifts in voter demographics or the influence of emerging media platforms on public opinion. Expanding the model’s validation across multiple election cycles could help verify its robustness and adaptability, ultimately refining its accuracy and reliability for future applications in election forecasting. 

\newpage

\appendix

# Appendix {-}


# FiveThirtyEight Licenses
[FiveThirtyEight's data sets](https://github.com/fivethirtyeight/data/tree/master/polls) are used and modified by us under the [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/).

# Overview of American Electon System

## Brief Description of American Federal Goverment

The American federal or national government is split into three branches, executive., legislative, and judical The executive branch includes the president and the military. The legislative branch include two subgroups, the House of Representatives and the Senate. These two subgroups create laws. Every state has two Senators and one Representative per approximately 750,000 people. The judicial branch is court system.

## What is the Electoral College {#sec-electoral-college}

The Electoral College is the system used in the United States to elect the president and vice president. Instead of a direct popular vote, each state is allocated a certain number of electors based on its representation in Congress (the total of its Senators and Representatives). When voters cast their ballots, they are actually voting for a slate of electors pledged to a candidate. The candidate who receives a majority of electoral votes (270 out of 538) wins the presidency. This system means that winning the popular vote in a state generally results in winning all of that state's electoral votes. The only exception are the states of Maine and Nebraska, who award electoral votes by congressional district, with two additional votes given to the statewide winner.

The Electoral College results in some states being unnecessary to campaign in, as their strong historical voting pasterns towards either Democrats or Republicans make them unlikely to change, regardless of campaign efforts. Therefore, for statisticians, polling information from these states may not be that useful when trying to predict the outcome of an election. On the other hand, states that can vote either Democratic or Republican (swing states) are immensely important when predicting an election. As a result, campaigns spend hundreds of millions of dollars campaigning and understanding voters there.

```{r}
#| label: fig-currentstateofelection
#| fig-cap: 2024 U.S. Presidential Election State Forecast Map
#| echo: false
#| warning: false

# Define your colors for each state (replace these colors with your desired ones)
state_colors_legend <- c(
  "Republican" = dark_red,      # Dark Red
  "Lean Republican" = light_red, # Light Red
  "Toss-Up" = purple,         # Purple
  "Lean Democrat" = light_blue,   # Light Blue
  "Democrat" = dark_blue,        # Dark Blue
  "Split By District" = "gray"
)

state_colors <- c(
  alabama = dark_red,
  arizona = purple,
  arkansas = dark_red,
  california = dark_blue,
  colorado = dark_blue,
  connecticut = dark_blue,
  delaware = dark_blue,
  `district of columbia` = dark_blue,
  florida = light_red,
  georgia = purple,
  idaho = dark_red,
  illinois = dark_blue,
  indiana = dark_red,
  iowa = light_red,
  kansas = dark_red,
  kentucky = dark_red,
  louisiana = dark_red,
  maine = "gray",
  maryland = dark_blue,
  massachusetts = dark_blue,
  michigan = purple,
  minnesota = light_blue,
  mississippi = dark_red,
  missouri = dark_red,
  montana = dark_red,
  nebraska = "gray",
  nevada = purple,
  `new hampshire` = light_blue,
  `new jersey` = dark_blue,
  `new mexico` = light_blue,
  `new york` = dark_blue,
  `north carolina` = purple,
  `north dakota` = dark_red,
  ohio = light_red,
  oklahoma = dark_red,
  oregon = dark_blue,
  pennsylvania = purple,
  `rhode island` = dark_blue,
  `south carolina` = dark_red,
  `south dakota` = dark_red,
  tennessee = dark_red,
  texas = light_red,
  utah = dark_red,
  vermont = dark_blue,
  virginia = light_blue,
  washington = dark_blue,
  `west virginia` = dark_red,
  wisconsin = purple,
  wyoming = dark_red
)

# Load the states map data
states_map <- map_data(map = "state")

state_colors <- setNames(state_colors, tolower(names(state_colors)))

color_to_status <- setNames(names(state_colors_legend), state_colors_legend)

states_map <- states_map %>%
  mutate(status = color_to_status[state_colors[region]])

plotclr <- c(dark_red, light_blue, purple, gray, light_blue, dark_blue)
legend_description <- c(
  "Likely Republican",
  "Lean Republican",
  "Swing States",
  "Lean Democrat",
  "Likely Democrat",
  "Split by District"
)


ggplot(states_map, aes(x = long, y = lat, group = group, fill = status)) +
  geom_polygon(color = "white", size = 0.2) +
  coord_map("albers", lat0 = 39, lat1 = 45) +
  scale_fill_manual(
    values = state_colors_legend,
    name = "Election Status"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    legend.title = element_text(size = 12, face = "bold"),
    legend.text = element_text(size = 10),
    axis.text = element_blank(),
    axis.title = element_blank(),
    panel.grid = element_blank()
  )

```

The state statuses presented in this map are based on evaluations from @cnn_electoral_nodate, @fox_news_2024_2024, and @nbc_news_choose_nodate, which are generally agreed upon within the American political community. These organizations assessed historical voting patterns and recent polling data to derive their conclusions.

Notably, Nebraska and Maine are indicated in gray due to their delegates being split by district. In Nebraska, the state overall is projected to lean Republican, with the first and third districts also strongly favoring Republican candidates, while the second district leans Democratic. In Maine, the overall expectation is a Democratic leaning, consistent with its first district, though the second district leans Republican. Furthermore, Alaska and Hawaii are not in the map. Alaska is strongly favoring Republicans while Hawaii strongly favors democrats.

The seven generally agreed upon swing states are Pennsylvania, North Carolina, Georgia, Arizona, Nevada, Wisconsin, and Michigan with Texas, Florida, Nebraska District Two, Maine District 2, and Minnesota as the next closest races.

# States Poll Count

```{r}
#| label: tbl-state-polls-count
#| tbl-cap: Polls included in Analysis Per State
#| echo: false
#| warning: false

states <- c(state.name, "National")
polls_per_state_tbl <- tibble(state = states)

polls_per_state <- cleaned_data %>% group_by(state) %>% 
  summarise(polls_count = n_distinct(poll_id))

polls_per_state_tbl <- polls_per_state_tbl %>% 
  left_join(polls_per_state, by = "state") %>% 
  mutate(polls_count = if_else(!is.na(polls_count), polls_count, 0)) %>% 
  select(state, polls_count)

polls_per_state_tbl %>% kable(col.names = c("State", "Number of Polls In Analysis"))


num_states_missing <- count(polls_per_state_tbl %>% 
                              filter(polls_count == 0) %>% 
                              select(state))

```

# The Ideal Survey

## Objective

The research team has developed a survey and distribution methodology with a hypothetical budget of $100,000. This objective is to create a polling system that accurately predicts the 2024 United States Election.

## Sampling Frame

Given the monetary constraint, the team's first decision is to mostly include swing states and districts. These are Nevada, Arizona, Wisconsin, Michigan, Pennsylvania, North Carolina, and Georgia. Swing states are the primary focus of the poll because they disproportionately affect the outcome of the election based on the explanation in @sec-electoral-college. However, the poll will also include Texas, Florida, and Minnesota, Nebraska District 2, and Maine District 2, as they have the potential to one by either party but are not as likely to change as the previously mentioned swing states. Nevertheless, it is important to note that to save costs, the team will focus on polling true swing states compared to Texas, Florida and Minnesota.

## Survey Sending Process

The team will then find and use multiple databases, such as @usps_customer_nodate, to find addresses, telephones, and emails of potential voters in those states. According to @pew_research_center_us_nodate, gathering this information reduces non-response and selection bias, as the team can contact the same individual through multiple mediums. Moreover, certain demographics may prefer specific communication forms; for instance, the elderly may prefer phone or paper mail polls over text or email. Additionally, to encourage individuals to complete the survey, one in every 50 participants will have a chance at winning $20.

## Sampling Methodology

Each state will have its own poll, but the polling methodology and the survey given will remain the same. Specifically, the polls will use stratified random sampling to make sure that participants reflect each state's counties in proportion to their population sizes. Ideally, the stratification would also ensure race, gender, age, and other socioeconomic factors are also accounted for. However, these factors are almost impossible to determine while sending the survey. Therefore, the research team decided to ask demographic questions directly inside the survey. After data collection, the team will weigh data based on demographics. For example, if a certain county has a 30% black population (this statistic will be determined from the US census), but only 15% of the survey participants are black, then the pollsters may decide to count each black participant's responses twice.

Furthermore, the team will aim to sample approximately 1,000 people from each true swing state. This sample size is large enough to provide reliable conclusions but not so large that it resembles sampling with replacement. For true swing states, the team aims to survey 1,000 individuals. However, if the final sample size falls short of this target, it is not considered a significant issue.

## Survey Implementation & Question Creation

The team's ideal survey will be made using @qualtrics_qualtrics_nodate. It will attempt to ensure four things: no leading questions, no question order bias, no answer order bias, and the survey should identify non-engaged participants. To identify participants who are not engaged, the research team has decided to add a worthless question. This question is extremely simple and clearly has one correct answer. Therefore, if a participant selects the wrong answer, they most likely are not engaged with the survey and their responses should be removed from the final data. An example of this is question number 7.

Order bias occurs when the sequence of questions subconsciously influences participants' responses. To prevent this, many questions should be randomized upon entry to the survey. However, some questions must follow a specific order, such as question 11, while others like questions 1-4 can be randomized. The team's hypothetical survey has not implemented this feature, though it is available with the @qualtrics_qualtrics_nodate paid plan.

Answer bias, like order bias, occurs when the order of answer choices influences participants' selections, with the first option often chosen more frequently. To prevent this, answer choices should be randomized upon survey entry. Questions 8 & 9 could benefit from this. Likewise, the team's hypothetical survey has not implemented this feature, but it is available in the @qualtrics_qualtrics_nodate paid plan.

A leading question occurs when a question is written in a way that suggests the user to give a certain answer. For example, "given that children are the future of our country, should we invest more money in their education". To prevent this, the researchers have ensured questions are written in a style where no unnecessary details or opinions are added.

## Survey Questions

Click this [link](https://qualtricsxm7d2hxss4j.qualtrics.com/jfe/form/SV_1Tu3PT2eUEa1Op8) for the @qualtrics_qualtrics_nodate survey.

List of all of the questions:

1. Select your race(s) (racial options where chosen based on @orvis_omb_2024)
   - White
   - Black or African American 
   - American Indian or Alaska Native 
   - Native Hawaiian or Pacific Islander 
   - Middle Eastern or North African
   - Asian 
   - Other 
   - Prefer not to answer

2. Please select your gender 
   - Male 
   - Female 
   - Non-binary 
   - Prefer not to say 

3. Please enter your age (in numbers) 
  - This is a text input field. Please note that this field has the auto-validation feature set to numbers in @qualtrics_qualtrics_nodate. As a result, participants can only input numbers in this field and are alert if they have not.

4. Please select the highest degree of education you have obtained
   - GED Certificate 
   - High School Diploma 
   - Undergraduate Degree 
   - Graduate Degree (Masters/Phd) 
   - None 
   - Other
   - Prefer not to answer

5. Are you a registered voter for the 2024 United States Presidential Election? 
   - Yes 
   - No 

6. Place yourself on the political spectrum 
   - Far Left 
   - Center Left 
   - Center 
   - Center Right 
   - Far Right 

7. Can pigs fly? 
   - Yes 
   - Maybe
   - No 
   - Prefer Not To Say 

8. What political party have you registered with? 
   - Republicans 
   - Democrats 
   - Green Party 
   - Libertarian 
   - Other
   - Independent (unregistered) 

9. Who will you vote for in the 2024 presidential election? 
   - Democrat - Kamala Harris 
   - Republican - Donald Trump 
   - Green Party - Gill Stein 
   - Libertarian - Chase Oliver 
   - Other
   - Will not vote

## Potential Problems with the Methodology And Polls

While the team's methodology and survey creates a robust system, there are potential issues. The reliance of weighting results based on a candidate's demographics can lead to error propagation. For instance, if a certain racial demographic population is only captured limitedly and that limited sample is far from representative, a few individuals in the population can have a large impact on the polls prediction of what candidate will win the state. There is also a selection bias in terms of the monetary reward. Potentially, people who like monetary rewards could be more likely to engage in the survey and could therefore exhibit certain voting or demographic characteristics that create an unrepresentative sample.

# Methodology of YouGov
YouGov's methodology documentations are seperated in two articles. The article by @bailey_how_2024 documents the methodology of the 2024 election projection, while the webpage on @yougov_methodology_nodate documents the general methodology of YouGov's prediction.

## Population, Frame, and Sample
As @bailey_how_2024 stated, the population covered by YouGov's MRP model is everyone in the national voter file, whether or not they belong to YouGov's panel. The national voter files are digital database built by commercial organizations with public government records of voters, as explained by @desilver_q_2018. Voter files indicates whether someone voted in a given election, thus YouGov's population covers all voters in previous US elections. 

YouGov’s sampling frame consists of its online panel members. These members are part of the SAY24 project, a collaboration between Stanford, Arizona State, and Yale Universities, as stated by @bailey_how_2024. YouGov collect information on respondents when they join their panel before they are invited to participate in the survey.

YouGov select the sample from the sampling frame based on their ability to match characteristics of the population of interest. YouGov interviews nearly 100,000 people in the first set of estimates. For the second set of estimates, YouGov didn't just start over with a new sample. They took the initial data from August and September and updated it with responses from more than 20,000 additional registered voters who were re-interviewed in late September and early October.

## Sample Recruitment
Panelists are recruited through various online channels, including advertisements and partnerships with websites [@yougov_methodology_nodate]. They must provide demographic details upon joining, which helps in selecting representative samples for each survey. When respondents complete a survey, they are awarded points that can be exchanged for money.

## Sampling Approach and Trade-offs
YouGov uses non-probability sampling due to the compensation, an approach where not every individual has an equal chance of selection [@yougov_methodology_nodate]. This method allows quick and cost-effective data collection. However, as @yougov_methodology_nodate writes the panelists must have an internet connection to participate. YouGov state that there is 95% of us population with internet access, thus the sample may be less representative of certain hard-to-reach populations, such as individuals with very slow internet access or without internet access. 

## Non-response Handling
YouGov apply statistical weighting to adjust for the differences between the sample and target population. The weight is based on demographic characteristics such as age, gender, race and presidential vote [@yougov_methodology_nodate]. Additionally, quality control measures exclude unreliable responses to improve data accuracy. The respondents are offered a small incentive to decrease the non-response and increase participation.

## Strengths and Weaknesses of the Questionnaire
YouGov's surveys are conducted online, which is very efficient for the respondents, and responses are weighted to enhance representativeness. The pollster can recruit a large amount of panelists because of the online format. Combining with online tracking technologies, the metadata provided by ther panelists can be verified easily.

As a non-probability sample, it might miss certain demographic groups not covered by the online population. While weighting improves accuracy, it cannot fully substitute the randomization found in probability sampling​. Additionaly, the categories in the surbey is oversimplified with bias. For instance, in the poll result published by @yougov_say24_nodate, gender is divided into Male and Female. Race is divided into White, Black, Hispanic and Other. This indicates a lack of representation. 


\newpage


# References


