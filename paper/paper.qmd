---
title: "Forecasting the 2024 U.S. Presidential Election through Poll Aggregation and Adjustments for Poll Quality"
subtitle: "A Hybrid Model Predicts a Narrow Lead for Kamala Harris in Swing States as Election Day Approaches"
author: 
  - Colin Sihan Yang
  - Lexun Yu
  - Siddharth Gowda
thanks: "Code and data are available at: [https://github.com/yulexun/uselection](https://github.com/yulexun/uselection)."
date: today
date-format: long
abstract: "This paper develops a hybrid model to forecast the 2024 U.S. presidential election by combining poll aggregation techniques with additional variables, including pollster reliability, sample size, timing relative to the election, geographic region, and polling methodology. By integrating these factors, our model accounts for variations in poll quality and regional differences in voter sentiment, providing a more stable and accurate prediction compared to traditional poll aggregation alone. Our results show that Kamala Harris holds a slight lead over Donald Trump in most swing states, with support levels stabilizing closer to election day. This approach reveals how poll quality and timing can significantly influence support predictions. This model offers a comprehensive tool for understanding the dynamics of voter sentiment and improving the accuracy of election predictions. "
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(here)
library(tibble)
library(knitr)
library(arrow)
library(rstanarm)
library(brms)
library(dplyr)
library(lubridate)
library(gridExtra)
library(Matrix)
library(rstan)

light_blue <- "#ADD8E6"
dark_blue <- "#6688CC"
light_red <- "#FF7777"
dark_red <- "#B22222"
purple <- "#9370DB"

cleaned_data <- read_parquet(here("data/02-analysis_data/cleaned_data.parquet"))
```


# Introduction {#sec-intro}
Election result forecasting has become an essential tool for analysts in political science and the public to predict the outcome of democratic process, such as the presidential election in the United States. Traditionally, individual polls have been used as a snapshot of voter sentiment, but they only reflect temporary changes in the performance of contestants, instead of a precise estimation of the election result. As discussed by @pasek and @blumenthal, the aggregation of multiple polls, or "poll-of-polls," has become a popular technique to reduce individual survey errors and provide more accurate election forecasts. However, conventional aggregation models often overlook dynamic election factors, including the credibility and quality variations among poll sources.

We build a hybrid election forecasting model following the strategies mentioned by @pasek. As @pasek described in their article, aggregation involves determining which surveys are worth including, as well as selecting, combining and averaging results from multiple polls to reduce individual biases and errors. Our model incorporates this approach, filtering polls from @fivethirtyeight based on a numeric grade that indicates each pollster's reliability.

The objective of this study is to predict the support percentages for the primary candidates, Kamala Harris and Donald Trump, using this hybrid model. We incorporate aggregation by filtering the polls on @fivethirtyeight by numeric grade that indicates pollster’s reliability. We also quantify other polling attributes, such as sample size, poll reliability scores, and polling methodology, which serve as predictors. Our results show that this model provides a more stable and accurate forecast than traditional poll aggregation alone. With this model, we generate a prediction that Kamala Harris will win the popular vote in 2024 and win the electoral college.

The remainder of this paper is structured as follows: @sec-data provides an overview of the data. @sec-model provides the modeling approach, including simple and multiple linear regression models and a Bayesian hierarchical model. We then present our results in @sec-result and discuss the implications, limitations, then we predict the election in @sec-prediction, and future research directions in @sec-discussion.

The data gathering and analysis is done in R [@citeR] with the following packages: knitr [@knitr], tidyverse [@tidyverse], ggplot2 [@ggplot2], dplyr [@dplyr], arrow [@arrow], here [@here], gridExtra [@gridextra], Matrix [@Matrix], Rstan [@Rstan] and lubridate [@lubridate].

## Estimand

The estimand for this research paper is the predicted support percentages for Kamala Harris and Donald Trump. The prediction is based on quantifying various polling factors, including sample size, poll scores, and poll methodology, which are used as predictors.

# Data {#sec-data}

## Measurement

The dataset we obtained from @fivethirtyeight is accumulated from multiple polls and surveys. According to FiveThirtyEight, they aggregate polling data conducted by other firms and organizations that meets their methodological and ethical standards [@morris_trump_2024]. The dataset has a list of questions in polls and surveys and their results. When a new poll is conducted, the poll is appended to the dataset. FiveThirtyEight assign the poll a poll_id, and each question is assigned a question_id. 

In each of the polls recorded by FiveThirtyEight, all options of each question are recorded in candidate_names, while the proportion of respondents choosing that option is recorded in pct as a percentage. The pct is our primary response variable. 

There are limitations in these measurements. The differences in sampling methods, wording in survey questions and systematic biases are all reflected in the outcome [@radcliffe_538s_2023]. Thus, in the dataset, FiveThirtyEight includes other variables, such as pollscore and sample_size for their prediction model in addition to the polling results for transparency and accuracy. As an example, pollscore indicates the pollster’s reliability rating, a low pollscore indicates a higher reliability rating [@silver_polls_2008].

In this dataset, each row represents a polling question that records the variables of interest. Each entry allows us to examine the real-world relationships between polling factors and the support percentage (`pct`) for the candidates Kamala Harris and Donald Trump. This dataset enables an analysis of how various polling characteristics influence the reported support levels for the candidates we are focused on.

These variables combined allow researchers to reliably analyze and predict the 2024 US election result over time.

## Data Examination {#sec-dataexplore}

The raw data from FiveThirtyEight contains 52 columns, all the column headers are displayed below: 
```{r}
#| echo: false
#| warning: false
raw_data <- read_csv(here("data/01-raw_data/president_polls.csv"))

# Get the column names of your data
column_names <- colnames(raw_data)

# Reshape the column names into a matrix with, for example, 4 columns
num_cols <- 3
column_matrix <- matrix(column_names, ncol = num_cols, byrow = TRUE)

# Convert the matrix to a data frame for kable
column_df <- as.data.frame(column_matrix)

# Display in multiple columns with kable
kable(column_df, format = "latex", booktabs = TRUE, col.names = NULL)
```

These columns can be categorized into three types, response variable, numeric predictors, and categorical predictors.

The response variable is:

- pct: The percentage of support or vote share that each candidate (Kamala Harris, Donald Trump, or third party alternatives) received in the poll.

Example of numeric predictors are:

- sample_size: The total number of respondents in each poll.
- pollscore: A numeric score representing the error and bias of the pollster. Negative numbers are better.
- numeric_grade: A numeric grade assigned to each pollster, reflecting pollster quality or reliability.

Example of categorical predictors are:

- state: The U.S. state in which the poll was conducted or targeted.
- methodology: The method used to conduct the poll (e.g., online, phone, in-person).
- party: The political party of the candidate (e.g., DEM for Democrat, REP for Republican).
- candidate_name: The name of the candidate (i.e. Kamala Harris, Donald Trump, or third party candidates).
- pollster: The name of the polling organization that conducted the poll.
- stage: The stage of the election (e.g., "general").

## Clean Data
The data cleaning process involves several steps to ensure the quality and relevance of the polling data. First, we filter the dataset to retain only poll results with a numeric grade of 2.7 or higher, indicating that the polls are considered reliable. Next, we address missing values in the state attribute: polls with NA in the state column are considered national polls.

We then create a new attribute, days taken from election, which represents the time gap between the poll's start date and the actual U.S. election date. Additionally, we filter the dataset to include only polls conducted after July 21, 2024, the date when Kamala Harris declared her candidacy. Finally, we remove any remaining rows that contain missing values to ensure a clean dataset.

### States included in anaylsis

After the data cleaning process, 21 states had no polling data. A table showing the number of polls for each state, including those without any polls, is provided in @tbl-state-polls-count.

This absence of polling data is not a significant concern due to the structure of the United States Electoral College (explained in detail in @sec-electoral-college). The states lacking polling data have consistently followed historical voting patterns, so predicting the winning candidate in those states is unnecessary.

## Cleaned data

The first 6 rows of the dataset are displayed in @tbl-cleaned-data.

```{r}
#| label: tbl-cleaned-data
#| tbl-cap: Sample of cleaned US election data 
#| echo: false
#| warning: false

cleaned_data |>
  select(pct, sample_size, pollscore, days_taken_from_election, state, methodology, candidate_name) |>
  head(6) |>
  kable(
    col.names = c("pct", "Sample Size", "pollscore", "Days taken from election","state", "methodology", "Candidate name"),
    booktabs = TRUE
    ) 
```

### Relationship Between Variables

#### State and Candidate Percentage

```{r}
#| label: fig-avg-pct-state
#| fig-cap: Historical State Voting Trends Are Maintined in 2024
#| echo: false
#| warning: false
#| 
cleaned_data <- cleaned_data %>% mutate(candidate_name =
  if_else(!(candidate_name %in% c("Kamala Harris", "Donald Trump")), "Other", candidate_name))

cleaned_data %>%
  filter(candidate_name %in% c("Kamala Harris", "Donald Trump")) %>%
  group_by(state, candidate_name) %>%
  summarise(avg_pct = mean(pct, na.rm = TRUE), .groups = "drop") %>%
  ggplot(mapping = aes(x = avg_pct, y = state, fill = candidate_name)) + 
  geom_col(position = "stack") +
  theme_minimal() +
  labs(
    x = "Average Polled Support (%)", 
    y = "State",
    fill = "Candidate"
  ) +
  scale_fill_manual(values = c("Kamala Harris" = light_blue, "Donald Trump" = light_red)) +
  theme_minimal() +
  theme(legend.position = "bottom") + 
  theme(axis.text.y = element_text(size = 5))

```

In @fig-avg-pct-state, historically Democratic states are polling for Kamala and historically Republican states are polling for Trump. Similarly, historically swing states also appear to be close, for instance Michigan (46.9% Trump, 47.5% Harris), Nevada (47.7% Trump, 47.9% Harris), and Pennsylvania (46.9% Trump, 48.2% Harris) all are close to an even split.

#### Poll Methodology and Candidate Percentage

```{r}
#| label: fig-avg-pct-methodology
#| fig-cap: Polls with more Methodologies have Less Variability.
#| echo: false
#| warning: false
#| 
cleaned_data <- cleaned_data %>% mutate(candidate_name =
  if_else(!(candidate_name %in% c("Kamala Harris", "Donald Trump")), "Other", candidate_name))

cleaned_data %>%
  filter(candidate_name %in% c("Kamala Harris", "Donald Trump")) %>%
  filter(!is.na(methodology)) %>% 
  ggplot(mapping = aes(x = methodology, y = pct)) + 
  geom_boxplot(width = 0.6, position = position_dodge(width = 0.8), outlier.shape = NA) +
  coord_flip() +
  theme_minimal() +
  labs(
    x = "Methodology",
    y = "Polled Support (%)",
    fill = "Candidate"
  ) +
  scale_fill_manual(values = c("Kamala Harris" = light_blue, "Donald Trump" = light_red)) +
  theme(legend.position = "bottom") + 
  theme(axis.text.y = element_text(size = 8))

```

@fig-avg-pct-methodology shows that polls utilizing multiple communication methods to reach voters tend to have lower interquartile ranges (IQRs) in their boxplots compared to those that rely on only one or two communication methods. It is important to note that statistical outliers are not visible in the graph to ensure better visibility.

#### Poll Score and Sample Size

```{r}
#| label: fig-pollscore-samplesize
#| fig-cap: Pollsters with Higher Pollscores Have Larger Sample Sizes in their Polls
#| echo: false
#| warning: false

# removing outliers and na's
graph_data <- cleaned_data %>% 
  filter(!is.na(pollscore) & !is.na(sample_size)) 

outlier_top <- (quantile(graph_data$sample_size, 0.75) + 
                  IQR(graph_data$sample_size)*1.5)

outlier_bottom <- (quantile(graph_data$sample_size, 0.25) - 
                  IQR(graph_data$sample_size)*1.5)

graph_data <- cleaned_data %>%
  filter(sample_size < outlier_top & sample_size > outlier_bottom)

graph_data %>% filter(!is.na(pollscore) & !is.na(sample_size)) %>% 
  ggplot(aes(x = pollscore, y = sample_size)) +
  geom_point(alpha = 0.33) + geom_smooth(method = "lm", se = FALSE, color = light_red) +
  labs(x = "Poll Score", y = "Poll Sample Size")

# this model is really here just for explain, this realtionship should not
# be treated as lienar
# cor(graph_data$sample_size, graph_data$pollscore)
# lm_score_sample <- lm(sample_size ~ pollscore, data = graph_data)
# summary(lm_score_sample)
```

@fig-pollscore-samplesize illustrates a weak positive correlation between a pollster's pollscore and the sample size of their poll. The figure also shows that most polls have a sample size of around 800 to 1200 participants. It is also important to note that a few polls with exceptionally large sample sizes were excluded from the graph due to their status as clear outliers.

#### Sample Size and Voter Percentage

```{r}
#| label: fig-pollsamplesize-canidate-pct
#| fig-cap: The Sample Size of a Poll does not impact a Candidate's Voting Percentage
#| echo: false
#| warning: false

cleaned_data %>% 
  filter(!is.na(sample_size) & !is.na(pct)) %>% 
  ggplot(aes(x = sample_size, y = pct)) +
  geom_point(alpha = 0.4) + 
  geom_smooth(method = "lm", se = FALSE, color = light_red) +
  labs(x = "Sample Size of the Poll",
       y = "Candidate Percentage") + facet_wrap(~candidate_name)

```

Based on @fig-pollsamplesize-canidate-pct, there does not seem to be a relationship between the sample size of a poll the voter percentage of a candidate.

#### Poll Score and Voter Percentage

```{r}
#| label: fig-pollscore-candidate-pct
#| fig-cap: More Reliable Pollsters Score Higher For Trump
#| echo: false
#| warning: false

cleaned_data %>% 
  filter(!is.na(pollscore) & !is.na(pct)) %>% 
  ggplot(aes(x = pollscore, y = pct)) +
  geom_point(alpha = 0.4) + 
  geom_smooth(method = "lm", se = FALSE, color = light_red) +
  labs(x = "Pollscore",
       y = "Candidate Percentage") + facet_wrap(~candidate_name)


```

@fig-pollscore-candidate-pct depicts the relationship between a pollster's poll score and a candidate's percentage. For Donald Trump, a negative correlation is observed, indicating that pollsters with lower poll scores tend to assign him a higher percentage of support. Conversely, Kamala Harris shows the opposite trend: as the poll score decreases, her percentage tends to rise. This suggests that more reliable polls, characterized by lower poll scores, report higher support for Trump compared to less reliable pollsters.

#### Days from Election and Voter Percentage

```{r}
#| label: fig-polldate-canidate-pct
#| fig-cap: Both Canidate Percentages Are Gaining Support Closer to the Election
#| echo: false
#| warning: false

cleaned_data %>% 
  filter(!is.na(days_taken_from_election) & !is.na(pct)) %>% 
  ggplot(aes(x = days_taken_from_election, y = pct)) +
  geom_point(alpha = 0.4) + 
  geom_smooth(method = "lm", se = FALSE, color = light_red) +
  labs(x = "Timing of Polls Relative to Election Day (Days Prior to the Election)",
       y = "Candidate Percentage") + facet_wrap(~candidate_name)

```

Based on @fig-polldate-canidate-pct, Trump and Harris are getting more votes in polls that are done closer to the election. This is a result of non-major candidate support decreasing. Specifically, Trump support is increasing at a faster rate than Harris. However, the polls in general show a slight lead for Harris throughout the last 100 days. 


# Model {#sec-model}

The goal of our modeling strategy is twofold. Firstly, we aim to accurately predict the support percentage (PCT) for Harris and Trump based on relevant poll data and key influencing factors. Secondly, we seek to evaluate the efficacy of different modeling approaches—from simple linear regression (SLR) to multiple linear regression (MLR) and Bayesian hierarchical models—to understand their predictive capabilities and assess the underlying relationships between variables. By comparing these models, we can determine which approach provides the most robust and reliable predictions, while considering the variability and potential uncertainty in the data.

## Model set-up {#sec-modelsetup}

The Bayesian model is implemented in R [@citeR] using the rstanarm package as described by @rstanarm. We run the model with the following specifications:

- Formula: $\text{pct} \sim \text{pollscore} + \text{days taken from election} + \text{sample size} + (1 | \text{methodology}) + (1 | \text{state})$

- Priors: Normal(0, 2.5) for all coefficients and intercept, Exponential(1) for $\sigma$

- Settings: Seed = 123, Cores = 4, Adapt delta = 0.95

We run the model in R [@citeR] using the `rstanarm` package of @rstanarm. We use the default priors from `rstanarm`.

## Basic SLR Model
```{r}
#| label: fig-model-pct-pollscore
#| fig-cap: "Pollscore has limited predictive accuracy for Kamala's voter percentage."
#| echo: false
#| warning: false

Harris_data = cleaned_data[cleaned_data$candidate_name == "Kamala Harris", ]
just_harris_data = Harris_data |> na.omit()
lm_model1 = lm(pct ~ pollscore, data = just_harris_data)
predictions = predict(lm_model1, just_harris_data)
# summary(lm_model1)

ggplot(just_harris_data, aes(x = pct, y = predictions)) +
  geom_point(color = 'blue') +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Single Linear Model (Only Pollscore Predictor) For Harris",
       x = "Actual Vote Percentage",
       y = "Predicted Vote Percentage") +
  theme_minimal()
```


We first set up an SLR model for predicting pct based on pollscore. The model is: 


$\hat{\text{pct}} = \beta_0 + \beta_1 \times \text{pollscore}$


In this SLR model, the response variable is pct and the only one predictor is the pollscore. @fig-model-pct-pollscore visualizes the relationship between the actual and predicted values of percentage support (pct) for Kamala Harris, based on a simple linear regression model with pollscore as the sole predictor. Blue points represent individual comparisons between actual and predicted values. The red dashed line represents the line of perfect prediction, where actual values would equal predicted values.

The primary concern lies in the evident dispersion of data points, which are widely spread and do not cluster closely around the line of perfect prediction (the dashed red line). This suggests that while pollscore may have some predictive capability, it does not adequately explain the variability in pct. The observed inconsistencies between actual and predicted values indicate that the relationship between pct and pollscore is not sufficiently captured by a linear model with just one predictor.

```{r}
#| label: fig-model-pct-sample_size
#| fig-cap: "Sample size has Limited Impact on Kamala's Voter Percentage"
#| echo: false
#| warning: false
lm_model1 = lm(pct ~ sample_size, data = just_harris_data)
predictions = predict(lm_model1, just_harris_data)
# summary(lm_model1)

ggplot(just_harris_data, aes(x = pct, y = predictions)) +
  geom_point(color = 'blue') +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Sample Size as Predictor Model for Harris",
       x = "Actual Vote Percentage",
       y = "Predicted Vote Percentage") +
  theme_minimal()

```

In @fig-model-pct-sample_size, the distribution of the points suggests that the model might not capture much variability in pct when sample size is the only predictor. This indicates that sample size has limited impact on pct.

## MLR model
```{r}
#| label: fig-model-pct-pollscore-etc
#| echo: false
#| warning: false
just_harris_data = Harris_data |> na.omit()
model_MLR = lm(pct ~ pollscore + days_taken_from_election + methodology + sample_size + state, data = just_harris_data)
# summary(model_MLR)
just_harris_data <- just_harris_data |> mutate(fitted_value = predict(model_MLR),  num_harris = round((pct / 100) * sample_size, 0))

predictions = predict(model_MLR, just_harris_data)


```

```{r}
#| label: fig-model-MLR
#| fig-cap: "The MLR Model captured significant variance in Kamala Harris' voter percentage"
#| echo: false
#| warning: false
ggplot(just_harris_data, aes(x = pct, y = fitted_value)) +
  geom_point(color = 'blue') +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Multiple Predictors Model For Kamala Harris",
       x = "Actual Voter Percentage",
       y = "Predicted Voter Percentage") +
  theme_minimal()
```

Next, we predict pct using pollscore, days teken from election, methodology, sample size, and state as predictors. The model is: 

$\hat{\text{pct}} = \beta_0 + \beta_1 \cdot \text{pollscore} + \beta_2 \cdot \text{days\_taken\_from\_election} + \beta_3 \cdot \text{methodology} + \beta_4 \cdot \text{sample\_size} + \beta_5 \cdot \text{state}$

@fig-model-MLR compares actual percentage values with the predicted pct values from the MLR model. Blue points represent individual comparisons between actual and predicted values. The red dashed line represents the line of perfect prediction, where actual values would equal predicted values.

The overall distribution of points suggests that the MLR model has captured a substantial portion of the variance in Kamala Harris' voter percentage. Most of the data points align well with the red dashed line, particularly within the middle range of pct values (between 40 and 60). This alignment indicates that the model performs well in this range, with predicted values correlating strongly with the actual observed outcomes. 

## Bayesian Model

Next, we set up a bayesian model as the following in accordance to @sec-modelsetup: 

\begin{align*}
\text{pct}_i &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_1 \cdot \text{pollscore}_i + \beta_2 \cdot \text{days\_taken\_from\_election}_i + \beta_3 \cdot \text{sample\_size}_i \\
+ u_{\text{methodology}[i]} + u_{\text{state}[i]} \\
\alpha &\sim \text{Normal}(0, 2.5) \\
\beta_j &\sim \text{Normal}(0, 2.5), \quad j = 1, 2, 3 \\
u_{\text{methodology}} &\sim \text{Normal}(0, \sigma_{\text{methodology}}) \\
u_{\text{state}} &\sim \text{Normal}(0, \sigma_{\text{state}})
\end{align*}

This hierarchical Bayesian model is designed to capture both the fixed effects of predictors and the random effects of grouping factors. The response variable is pct (percentage of support), predictors are pollscore, days taken from election, and sample size. A normal prior with mean 0 and standard deviation 2.5 is set for all coefficients and the intercept, scaled automatically.


```{r}
#| label: fig-bayesian-model
#| fig-cap: "Posterior Predictive Check for the Bayesian Model Predicting pct"
#| echo: false
#| warning: false
bayesian_model_1 = readRDS(here("models/bayesian_model_1.rds"))

pp_check(bayesian_model_1)
```


### Posterior Predictive Checks 
@fig-bayesian-model displays the results of a posterior predictive check (PPC) for a Bayesian model using the pp_check() function. 

The observed data (y) follows a distinct, symmetrical distribution centered around a specific value, suggesting a well-defined peak with tapering on both sides. The replicated distributions (y_rep), shown as multiple thin lines, align with the shape of the observed distribution, indicating that the Bayesian model has captured the main characteristics of the data. However, the variability in the y_rep lines highlights the degree of uncertainty inherent in the model’s predictive capability.

The fact that the y_rep curves closely match the overall pattern of the actual y suggests that the model performs well in replicating the observed data. Minor discrepancies or deviations between the y and y_rep might imply areas where the model could be fine-tuned or adjusted to improve accuracy. Overall, this PPC indicates that the model provides a decent fit to the data, with some variability accounted for in the predictive samples.

### Train Test Validation
we implemented a train-test split validation approach to evaluate the performance of a Bayesian hierarchical model predicting percentage support (pct) for Kamala Harris. The dataset was divided into training (70%) and test (30%) sets to fit and validate the model, respectively. We used the stan_glmer function to fit the model on the training data, with predictors including pollscore, days_taken_from_election, sample_size, and random intercepts for methodology and state. After training, we generated predictions on the test set and calculated an R squared value to quantify the model's predictive accuracy. 
```{r}
#| echo: false
#| warning: false
# Split data into training and test sets
set.seed(123)
# train_indices <- sample(seq_len(nrow(just_harris_data)), size = 0.7 * nrow(just_harris_data))
# train_data <- just_harris_data[train_indices, ]
# test_data <- just_harris_data[-train_indices, ]
train_data = read_parquet(here("data/02-analysis_data/train_data.parquet"))
test_data = read_parquet(here("data/02-analysis_data/test_data.parquet"))

# Fit the model on the training data
bayesian_model_train = readRDS(here("models/bayesian_model_train.rds"))
# bayesian_model_train <- stan_glmer(
#   formula = formula,
#   data = train_data,
#   family = gaussian(),
#   prior = priors,
#   prior_intercept = priors,
#   seed = 123,
#   cores = 4,
#   adapt_delta = 0.95
# )

# Predict on the test data and check performance
predictions <- posterior_predict(bayesian_model_train, newdata = test_data)
```

\begin{align}
SS_{\text{total}} = \sum_{i=1}^{n} \left( y_i - \bar{y} \right)^2\\
SS_{\text{residual}} = \sum_{i=1}^{n} \left( y_i - \hat{y}_i \right)^2\\
R^2 = 1 - \frac{SS_{\text{residual}}}{SS_{\text{total}}}\\
\text{R-squared} = 0.700563202695799
\end{align}

```{r}
#| warning: false
#| echo: false
ss_total <- sum((test_data$pct - mean(test_data$pct))^2)
ss_residual <- sum((test_data$pct - colMeans(predictions))^2)
r_squared <- 1 - (ss_residual / ss_total)
# "R-squared: 0.700563202695799"
```

Where:\
\begin{itemize}
    \item \( y_i \) represents the actual values,
    \item \( \bar{y} \) is the mean of the actual values,
    \item \( \hat{y}_i \) are the predicted values,
    \item \( SS_{\text{total}} \) is the total sum of squares,
    \item \( SS_{\text{residual}} \) is the sum of squared residuals.
\end{itemize}

We could see that the R squared value equal to the 0.700563202695799 suggesting that the model captures a substantial portion of the variability in the data and demonstrates that the model performs well in explaining the variability of the pct in the test data, reflecting strong predictive capabilities.

```{r}
#| label: fig-bayesian-model-actual-predict
#| fig-cap: "The MLR Model Accruatley Predicts the Testing Dataset"
#| echo: false
#| warning: false
plot(test_data$pct, colMeans(predictions), 
     xlab = "Actual Voter Percentage Values", 
     ylab = "Predicted Voter Percentage Values", 
     main = "Multiple Predictors Kamala Harris Bayesian Model")
abline(0, 1, col = "red")
```

 By visual checking actual pct and predicted pct plot in @fig-bayesian-model-actual-predict, the points are plotted against a 45-degree line, which represents the ideal scenario where predicted values match the actual values perfectly.

```{r}
#| label: fig-bayesian-model-residual
#| fig-cap: "MLR Residuals Appear to Show No Trends"
#| echo: false
#| warning: false
residuals <- test_data$pct - colMeans(predictions)
plot(colMeans(predictions), residuals, 
     xlab = "Predicted Values", 
     ylab = "Residuals", 
     main = "Residual for the Bayesian Hierarchical Model Predictions")
abline(h = 0, col = "red")
```

In addition, the residual plot @fig-bayesian-model-residual is fairly centered around zero with no major trend, suggesting that the model is not heavily biased in its predictions. 

In conclusion, from the visual checks from the predicted vs. actual plot and the residual plot, there is no robust evidence that the Bayesian model is overfitting. It appears to generalize well to the data it was trained on without showing signs of capturing noise or irrelevant patterns.  

### Model justification

We chose sample size, poll score, and days taken from the election as predictors for building the model based on the visual trends observed in the plots. As shown in @fig-pollsamplesize-canidate-pct, @fig-pollscore-candidate-pct and @fig-polldate-canidate-pct in @sec-data, the relationship between percentage and sample size shows that larger sample sizes correspond to more stable percentages, indicating that sample size could significantly impact prediction accuracy. The plot of percentage versus poll score shows a negative trend for Donald Trump and a positive trend for Kamala Harris, suggesting that poll scores are consistently influential in explaining variations in percentage. Additionally, the percentage versus days taken plot indicates that the proximity to the election affects the percentage, with a slight boost towards major candidates. These observed patterns imply that these variables can effectively capture key factors influencing pct and are suitable for model inclusion.

To predict the support percentage (PCT) for Harris and Trump, we employed a modeling strategy involving Simple Linear Regression (SLR), Multiple Linear Regression (MLR), and a Bayesian hierarchical model. The SLR model provided a foundational analysis of the relationship between PCT and pollscore, highlighting its limited predictive power due to a lack of complexity. The MLR model improved on this by incorporating additional predictors such as days taken from the election, sample size, methodology, and state, enhancing its predictive accuracy and capturing interactions among variables. The Bayesian hierarchical model further refined our approach, incorporating prior knowledge and accounting for variability at group levels (e.g., methodology and state) through random intercepts. This model provided robust uncertainty quantification through credible intervals and demonstrated strong predictive performance with an R-squared value around 0.70, indicating it effectively explained data variability. By leveraging the strengths of these models, particularly the Bayesian approach’s interpretability and robustness, we achieved reliable predictions and a deeper understanding of the factors influencing support percentages.

The team also built a Bayesian MLR using the same approach and formula for Trump. It is in section @sec-trumpmodel.


# Result {#sec-result}

## Harris Voter Prediction Model 
```{r}
#| label: fig-bayesian-model-CI
#| fig-cap: "Kamala Harris is Predicted to get Approximatley 52% of Popular Vote"
#| echo: false
#| warning: false
# Plot random effects
plot(bayesian_model_1, pars = "(Intercept)", prob = 0.95, 
     xlab = "Kamala Harris Popular Vote Percentage",
     title = "Estimated Posterior Distribution for the Intercept Parameter in the Bayesian Model")
```


@fig-bayesian-model-CI displays the estimated posterior distribution for the intercept parameter in the Bayesian model. The intercept represents the baseline level of support, expressed as a percentage, for Kamala Harris in the U.S. election data used in the analysis. The plot features a point estimate (depicted by the central dot) that indicates the mean or median of the posterior distribution for the intercept, and a horizontal line showing the 95% credible interval, which signifies the range within which the true value of the intercept is likely to fall with 95% probability.

The estimated intercept, which represents the baseline percentage of support for Kamala Harris in the U.S. election data analyzed, is centered around 52%, with a credible interval spanning from 49.5% to 53.5%. The narrow width of the interval implies a certain degree of confidence in the estimate, indicating that the data used in the model provided a clear signal for the intercept's value. 

```{r}
#| label: tbl-bayesian-model-result-summary
#| tbl-cap: "Bayesian Model Result Summary"
#| echo: false
#| warning: false

predictions2 = posterior_predict(bayesian_model_1, newdata = just_harris_data)

predicted_means = colMeans(predictions2)
predicted_intervals = apply(predictions2, 2, quantile, probs = c(0.025, 0.975))

# Combine the results with the actual data
result_summary = data.frame(
  Actual_PCT = just_harris_data$pct,
  Predicted_PCT = predicted_means,
  Lower_CI = predicted_intervals[1, ],
  Upper_CI = predicted_intervals[2, ]
)

kable(head(result_summary, 10), 
      col.names = c("Acutal Kamala Percentage", 
                    "Predicted Percentage", 
                    "Lower Bound Credible Interval",
                    "Upper bound Credible Interval"))
```

```{r}
#| label: fig-bayesian-model-result-CI
#| fig-cap: "The Model Shows a Strong Alignment Between Actual and Predicted Vote Percentages"
#| echo: false
#| warning: false
ggplot(result_summary, aes(x = Actual_PCT, y = Predicted_PCT)) +
  geom_point() +
  geom_errorbar(aes(ymin = Lower_CI, ymax = Upper_CI), width = 0.2, color = "blue") +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  labs(
    title = "U.S. Election Prediction Results",
    x = "Actual Percentage",
    y = "Predicted Percentage"
  )
```

@fig-bayesian-model-result-CI shows that the model's predictive performance for Harris's election data appears robust, demonstrating a strong alignment between actual and predicted vote percentages. Most of the data points cluster around the line, indicating reliable predictions, some variance is evident, particularly in the lower and higher actual percentage ranges. This suggests that while the model performs well on average, it may have larger uncertainty in its predictions at the extremes. @tbl-bayesian-model-result-summary also highlights that most predicted values align closely with actual values, supporting the model’s robustness in capturing the overall trend of election outcomes.

The team has provided results for a Trump Bayesian MLR model. It is in section @sec-trumpmodelresults.

\newpage

# Election Prediction {#sec-prediction}

Our prediction process consists of two primary components. First, we develop models for both Trump and Harris based on the variables outlined in @sec-model and @sec-trumpmodel. This involves partitioning the dataset into training and testing subsets. Next, we further divide the testing dataset by state. We then input this test data into the respective models to generate predictions. By averaging these predictions, we can calculate the expected voter percentage for each candidate in each state. The candidate with a higher percentage is deemed the winner for that state.

We generated predictions for the following states: Arizona, Nevada, Georgia, Pennsylvania, Michigan, Minnesota, Wisconsin, Florida, Texas, Maine CD-2, Nebraska CD-2, New Hampshire, Ohio, Virginia, North Carolina, and Iowa. Winners for other states were determined based on historical trends and predictions from sources like @cnn_electoral_nodate. Most states without predictions are strongly Republican or Democratic, so their absence is not expected to significantly impact prediction validity.
```{r}
#| echo: false
#| warning: false
set.seed(123)
train_data_trump <- read_parquet(here("data/02-analysis_data/train_data_trump.parquet"))
test_data_trump <- read_parquet(here("data/02-analysis_data/test_data_trump.parquet"))
bayesian_model_train_trump <- readRDS(here("models/bayesian_model_train_trump.rds"))
```

```{r}
#| echo: false
#| warning: false
#| label: tbl-electionprediction
#| tbl-cap: "Kamala Harris Wins Most of the Swing States"
# Split data into training and test sets


train_data_harris <- read_parquet(here("data/02-analysis_data/train_data_harris.parquet"))
test_data_harris <- read_parquet(here("data/02-analysis_data/test_data_harris.parquet"))
bayesian_model_train_harris <- readRDS(here("models/bayesian_model_train_harris.rds"))

relevant_states <- c("Arizona", "Nevada", "Georgia", "Pennsylvania", "Michigan", "Minnesota",
                     "Wisconsin", "Florida", "Texas", "Maine CD-2", "Nebraska CD-2", 
                     "New Hampshire", "Ohio", "Virginia", "North Carolina", "Iowa")

# Initialize an empty tibble to store results
swing_state_predictions <- tibble(
  state = character(),
  harris_predicted_pct = numeric(),
  trump_predicted_pct = numeric(),
  winner = character()
)

for(state in relevant_states) {
  #print(state, state %in% unique(test_data_trump), state %in% unique(test_data_harris))
  test_data_state_harris <- test_data_harris[test_data_harris$state == state, ]
  test_data_state_trump <- test_data_trump[test_data_trump$state == state, ]
  
  predictions_state_harris <- posterior_predict(bayesian_model_train_harris, newdata = test_data_state_harris)
  predictions_state_trump <- posterior_predict(bayesian_model_train_trump, newdata = test_data_state_trump)
  
  avg_predicted_pct_state_harris <- mean(predictions_state_harris)
  avg_predicted_pct_state_trump <- mean(predictions_state_trump)
  
  # Add a new row to the results tibble
  new_row <- tibble(
    state = state,
    harris_predicted_pct = avg_predicted_pct_state_harris,
    trump_predicted_pct = avg_predicted_pct_state_trump,
    winner = ifelse(trump_predicted_pct > harris_predicted_pct, "Trump", "Harris")
  )
  # print(state)
  # print(avg_predicted_pct_state_harris)
  # print(avg_predicted_pct_state_trump)
  # 
  swing_state_predictions <- bind_rows(swing_state_predictions, new_row)
}

swing_state_predictions <- swing_state_predictions %>% arrange(state) %>% unique()

# View the results
swing_state_predictions %>% 
  kable(col.names = 
          c("State", "Harris Predicted Percentage", 
            "Trump Predicted Percentage", "State Winner"))
```


```{r}
#| label: fig-electionpredictionmap
#| fig-cap: "Kamala Harris is Predicted to be the 47th President of the United States"
#| echo: false
#| warning: false

# Define your colors for each state (replace these colors with your desired ones)
state_colors_legend <- c(
  "Republican" = "red",      # Dark Red
  "Democrat" = "blue",        # Dark Blue
  "Split By District" = "gray"
)

purple = "purple"
light_blue = "green"
light_red = "orange"

state_colors <- c(
  alabama = "red",
  arizona = ifelse((swing_state_predictions %>% 
                        filter(state == "Arizona") %>% 
                        select(winner)) == "Trump", "red", "blue"),
  arkansas = "red",
  california = "blue",
  colorado = "blue",
  connecticut = "blue",
  delaware = "blue",
  `district of columbia` = "blue",
  florida = ifelse((swing_state_predictions %>% 
                        filter(state == "Florida") %>% 
                        select(winner)) == "Trump", "red", "blue"),
  georgia = ifelse((swing_state_predictions %>% 
                        filter(state == "Georgia") %>% 
                        select(winner)) == "Trump", "red", "blue"),
  idaho = "red",
  illinois = "blue",
  indiana = "red",
  iowa = ifelse((swing_state_predictions %>% 
                        filter(state == "Iowa") %>% 
                        select(winner)) == "Trump", "red", "blue"),
  kansas = "red",
  kentucky = "red",
  louisiana = "red",
  maine = "gray",
  maryland = "blue",
  massachusetts = "blue",
  michigan = ifelse((swing_state_predictions %>% 
                        filter(state == "Michigan") %>% 
                        select(winner)) == "Trump", "red", "blue"),
  minnesota = ifelse((swing_state_predictions %>% 
                        filter(state == "Minnesota") %>% 
                        select(winner)) == "Trump", "red", "blue"),
  mississippi = "red",
  missouri = "red",
  montana = "red",
  nebraska = "gray",
  nevada = ifelse((swing_state_predictions %>% 
                        filter(state == "Nevada") %>% 
                        select(winner)) == "Trump", "red", "blue"),
  `new hampshire` = ifelse((swing_state_predictions %>% 
                        filter(state == "New Hampshire") %>% 
                        select(winner)) == "Trump", "red", "blue"),
  `new jersey` = "blue",
  `new mexico` = "blue",
  `new york` = "blue",
  `north carolina` = ifelse((swing_state_predictions %>% 
                        filter(state == "North Carolina") %>% 
                        select(winner)) == "Trump", "red", "blue"),
  `north dakota` = "red",
  ohio = ifelse((swing_state_predictions %>% 
                        filter(state == "Ohio") %>% 
                        select(winner)) == "Trump", "red", "blue"),
  oklahoma = "red",
  oregon = "blue",
  pennsylvania = ifelse((swing_state_predictions %>% 
                        filter(state == "Pennsylvania") %>% 
                        select(winner)) == "Trump", "red", "blue"),
  `rhode island` = "blue",
  `south carolina` = "red",
  `south dakota` = "red",
  tennessee = "red",
  texas = ifelse((swing_state_predictions %>% 
                        filter(state == "Texas") %>% 
                        select(winner)) == "Trump", "red", "blue"),
  utah = "red",
  vermont = "blue",
  virginia = ifelse((swing_state_predictions %>% 
                        filter(state == "Virginia") %>% 
                        select(winner)) == "Trump", "red", "blue"),
  washington = "blue",
  `west virginia` = "red",
  wisconsin = ifelse((swing_state_predictions %>% 
                        filter(state == "Wisconsin") %>% 
                        select(winner)) == "Trump", "red", "blue"), 
  wyoming = "red")

# Load the states map data
states_map <- map_data(map = "state")

state_colors <- setNames(state_colors, tolower(names(state_colors)))

color_to_status <- setNames(names(state_colors_legend), state_colors_legend)

states_map <- states_map %>%
  mutate(status = color_to_status[state_colors[region]])

plotclr <- c("red", gray, "blue")
legend_description <- c(
  "Republican",
  "Democrat",
  "Split by District"
)


ggplot(states_map, aes(x = long, y = lat, group = group, fill = status)) +
  geom_polygon(color = "white", size = 0.2) +
  coord_map("albers", lat0 = 39, lat1 = 45) +
  scale_fill_manual(
    values = state_colors_legend,
    name = "Election Status"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    legend.title = element_text(size = 12, face = "bold"),
    legend.text = element_text(size = 10),
    axis.text = element_blank(),
    axis.title = element_blank(),
    panel.grid = element_blank()
  )

```

According to Figure @fig-currentstateofelection, Kamala Harris is predicted to be the 47th President of the United States. There are also a few states with predictions not visible on the map, we will describe those predictions below. 

In Maine, Harris is projected to win the state’s overall delegates and District 1, while District 2 is expected to go to Trump. In Nebraska, Trump is expected to win the state’s delegates along with Districts 1 and 3, while Harris is predicted to win District 2. Additionally, Trump is projected to win Alaska, and Harris is expected to win Hawaii. 

Overall, the predictions indicate that Harris will receive 298 delegates, while Trump will receive 240 delegates. 

# Discussion {#sec-discussion} 

 

## Summary of Findings 

 

In this paper, we developed a hybrid election forecasting model to predict the 2024 U.S. presidential election results. Our model provides a stable and accurate forecast compared to traditional methods by combining poll-of-polls aggregation with additional factors such as poll score, sample size, days from the election, state, and methodology.  

 

Our approach in models applies simple linear regression, multiple linear regression and Bayesian hierarchical modeling that includes both fixed and random effects, capturing variations in poll reliability. The results show that a model accounting for poll quality, timing, and methodology can produce forecasts that are more resilient to biases found in individual polls. 

 

## Polling and Election Dynamics 

 

One of the main takeaways from this study is the impact of poll quality on election forecasts. We weight and filter the polls by their reliability. This approach highlights the importance of screening and adjusting for poll reliability in election forecasts, particularly in a high-stakes environment where even minor polling biases can influence public perception and campaign strategies. 

 

## Sources of Bias in the Polls 

 

Based on the figures in @sec-data, the team identified three sources of bias beyond sample size and state: methodology, the timing of polls relative to the election date, and poll scores. From the analysis in Figure @fig-avg-pct-methodology, we found that polls employing a limited number of methods to reach and communicate with voters exhibited greater variability. This finding is plausible, as different demographic groups often prefer distinct communication methods; for example, older voters may favor phone surveys, while younger voters might prefer online surveys. Additionally, using multiple communication methods provides pollsters with more opportunities to contact potential voters, thereby reducing non-response bias. Consequently, it is essential to incorporate methodology as a variable in the model, and we recommend that pollsters adopt diverse communication methods whenever possible. 

 

From @fig-polldate-canidate-pct, the team observed a decline in third-party candidate support as the election approached. Early in the election cycle, voters may entertain the idea of a viable third-party candidate, but as the election nears, interest in third-party options diminishes. This trend may be worsened by decreased participation in polls further from the election day. As a result, the people who do participate in those earlier polls are likely to be more informed and passionate, which are qualities that may be associated with support for third-party candidates. Additionally, the withdrawal of a prominent third-party candidate, Robert F. Kennedy Jr., on August 23, 2024 (74 days from the election), could influence this dynamic. However, we find the earlier reasons more compelling, as the data in Figure @fig-polldate-canidate-pct do not indicate a significant drop in third-party support at that time, but rather a gradual decline. 

 

Finally, as illustrated in Figure @fig-pollscore-candidate-pct, more reliable polls tend to show a bias favoring Trump over Harris—not necessarily indicating a guaranteed victory for Trump, but rather suggesting he may outperform expectations in many polls. It is important to note that the FiveThirtyEight poll score system is partly based on historical accuracy. Given Trump's overperformance in the polls during the 2016 and 2020 elections, this scoring may be overcorrecting for past inaccuracies. This potential undervaluation of Trump’s support could benefit the Harris campaign, by providing a warning to not repeat previous election mistakes.



## Limitations of the Model {#sec-model-limitations} 

 

First, any systematic biases inherent in those polls can carry over into the model’s predictions because we are using existing poll data. For instance, if certain demographic groups are consistently underrepresented, our model may not fully capture their impact on election outcomes.  

Second, the model only reflects current voting dynamics as it is based on historical data. This model does not account for short-term variability such as Biden's exit. This limitation is important in the final weeks before an election when small shifts in polling data can produce exaggerated effects. 

On top of that, the results can be sensitive to the choice of priors. We choose Normal(0, 2.5) as we believe it could stabilize the estimates by being informative enough to guide the posterior distribution without overpowering the data. However, if the chosen prior does not align well with the true underlying distribution, it may lead to biased results or overly wide credible intervals. 

Also, as it is a model based on a dataset, it is challenging to capture all the real-life features and dynamics. The model might not include all relevant predictors or interactions due to limitations in data availability or complexity, which can lead to incomplete representations of the factors influencing voting behavior. Additionally, without strong regularization techniques, the model may become prone to overfitting, particularly when using complex hierarchical structures or including numerous predictors. This overfitting can reduce the model’s generalizability to new or unseen data.

Furthermore, the incorporation of both poll score and sample size into the model may lead to the potential for double counting the sample size. This concern was raised in Nate Silver's article [@silver_polls_2008]. As illustrated in @fig-pollscore-samplesize, there is a positive correlation between poll score and sample size. However, this correlation is weak. Additionally, given that pollsters with negative poll scores are generally more reliable than those with positive scores, we would expect the opposite relationship to hold true for sample size to even have a possibility of being double counted.

Moreover, errors and offsets inherent in polling data, such as response bias, nonresponse adjustments, and sampling variability, can propagate into the model’s results. These aspects introduce an additional layer of uncertainty that can affect the model's reliability and predictive performance. While Bayesian methods provide a robust framework to incorporate uncertainty, the final outputs must be interpreted cautiously, acknowledging these underlying limitations. 

## Limitation of the Prediction 

In @sec-prediction, the team predicted that Kamala Harris would win 298 delegates while Donald Trump would get 240 delegates. However, this prediction has several limitations. First, the prediction will inherit all of the limitations of the model it (model limitations in @sec-model-limitations). Additionally, the predictions are based on averaging results from polling data for each state, which can lead to propagated prediction errors. 

Also, each state has a limited number of polls available. While the overall analysis considered a substantial number of polls, individual states often had fewer than 20 polls, with even the largest states having only 27. Moreover, as noted in section @sec-prediction, the analysis focused solely on contested races, specifically swing states. Therefore, many states were assigned to a candidate based on strong historical trends and not from predictions. Although this approach is reasonable, it may overlook current trends that could potentially shift a state's vote from Republican to Democrat or vice versa. 

## Future Directions 

To enhance the reliability and robustness of this forecasting model, future work should focus on validating its performance across multiple election cycles. We include previous year's polling dataset in this R project. By applying the model to past elections, researchers can assess its accuracy in different political contexts and electoral dynamics. This approach would show any limitations specific to certain election conditions, such as shifts in voter demographics or the influence of emerging media platforms on public opinion. Expanding the model’s validation across multiple election cycles could help verify its robustness and adaptability, refining its accuracy and reliability for future applications in election forecasting.  

\newpage

\appendix

# Appendix {-}


# FiveThirtyEight Licenses
[FiveThirtyEight's data sets](https://github.com/fivethirtyeight/data/tree/master/polls) are used and modified by us under the [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/).



# Overview of American Electon System

## Brief Description of American Federal Government 

The American federal or national government is split into three branches, executive, legislative, and judicial. The executive branch includes the president and the military. The legislative branch includes two subgroups, the House of Representatives, and the Senate. These two subgroups create laws. Every state has two Senators and one Representative per approximately 750,000 people. Finally, the judicial branch is the court system. 

## What is the Electoral College {#sec-electoral-college} 

The Electoral College is the system used in the United States to elect the president and vice president. Instead of a direct popular vote, each state is allocated a certain number of electors based on its representation in Congress (the total of its Senators and Representatives). When voters vote, they are voting for what candidate they want their electors to vote for. The candidate who receives the most electoral votes (270 out of 538) wins the presidency. This system means that winning the popular vote in a state results in winning all that state's electoral votes. The only exception are the states of Maine and Nebraska, who award electoral votes by congressional district, with two additional votes given to the statewide winner. 

The Electoral College results in some states being unnecessary to campaign in, as their strong historical voting patterns towards either Democrats or Republicans make them unlikely to change, regardless of campaign efforts. Therefore, for statisticians, polling information from these states may not be that useful when trying to predict the outcome of an election. On the other hand, states that can vote either Democratic or Republican (swing states) are immensely important when predicting an election. As a result, campaigns spend hundreds of millions of dollars campaigning and understanding voters there. 
```{r}
#| label: fig-currentstateofelection
#| fig-cap: 2024 U.S. Presidential Election State Forecast Map
#| echo: false
#| warning: false
light_blue <- "#ADD8E6"
dark_blue <- "#6688CC"
light_red <- "#FF7777"
dark_red <- "#B22222"
purple <- "#9370DB"
# Define your colors for each state (replace these colors with your desired ones)
state_colors_legend <- c(
  "Republican" = dark_red,      # Dark Red
  "Lean Republican" = light_red, # Light Red
  "Toss-Up" = purple,         # Purple
  "Lean Democrat" = light_blue,   # Light Blue
  "Democrat" = dark_blue,        # Dark Blue
  "Split By District" = "gray"
)

state_colors <- c(
  alabama = dark_red,
  arizona = purple,
  arkansas = dark_red,
  california = dark_blue,
  colorado = dark_blue,
  connecticut = dark_blue,
  delaware = dark_blue,
  `district of columbia` = dark_blue,
  florida = light_red,
  georgia = purple,
  idaho = dark_red,
  illinois = dark_blue,
  indiana = dark_red,
  iowa = light_red,
  kansas = dark_red,
  kentucky = dark_red,
  louisiana = dark_red,
  maine = "gray",
  maryland = dark_blue,
  massachusetts = dark_blue,
  michigan = purple,
  minnesota = light_blue,
  mississippi = dark_red,
  missouri = dark_red,
  montana = dark_red,
  nebraska = "gray",
  nevada = purple,
  `new hampshire` = light_blue,
  `new jersey` = dark_blue,
  `new mexico` = light_blue,
  `new york` = dark_blue,
  `north carolina` = purple,
  `north dakota` = dark_red,
  ohio = light_red,
  oklahoma = dark_red,
  oregon = dark_blue,
  pennsylvania = purple,
  `rhode island` = dark_blue,
  `south carolina` = dark_red,
  `south dakota` = dark_red,
  tennessee = dark_red,
  texas = light_red,
  utah = dark_red,
  vermont = dark_blue,
  virginia = light_blue,
  washington = dark_blue,
  `west virginia` = dark_red,
  wisconsin = purple,
  wyoming = dark_red
)

# Load the states map data
states_map <- map_data(map = "state")

state_colors <- setNames(state_colors, tolower(names(state_colors)))

color_to_status <- setNames(names(state_colors_legend), state_colors_legend)

states_map <- states_map %>%
  mutate(status = color_to_status[state_colors[region]])

plotclr <- c(dark_red, light_blue, purple, gray, light_blue, dark_blue)
legend_description <- c(
  "Likely Republican",
  "Lean Republican",
  "Swing States",
  "Lean Democrat",
  "Likely Democrat",
  "Split by District"
)


ggplot(states_map, aes(x = long, y = lat, group = group, fill = status)) +
  geom_polygon(color = "white", size = 0.2) +
  coord_map("albers", lat0 = 39, lat1 = 45) +
  scale_fill_manual(
    values = state_colors_legend,
    name = "Election Status"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    legend.title = element_text(size = 12, face = "bold"),
    legend.text = element_text(size = 10),
    axis.text = element_blank(),
    axis.title = element_blank(),
    panel.grid = element_blank()
  )

```

The state statuses presented in this map are based on evaluations from @cnn_electoral_nodate, @fox_news_2024_2024, and @nbc_news_choose_nodate, which are agreed upon within the American political community. These organizations assessed historical voting patterns and recent polling data to derive their conclusions. 

Notably, Nebraska and Maine are indicated in gray due to their delegates being split by district. In Nebraska, the state overall is projected to lean Republican, with the first and third districts also strongly favoring Republican candidates, while the second district leans Democratic. In Maine, the overall state is a Democratic leaning, and so is its first district, but the second district leans Republican. Furthermore, Alaska and Hawaii are not on the map. Alaska is strongly favoring Republicans while Hawaii strongly favors democrats. 

The seven agreed upon swing states are Pennsylvania, North Carolina, Georgia, Arizona, Nevada, Wisconsin, and Michigan with Texas, Florida, Nebraska District Two, Maine District 2, Iowa, and Minnesota as the next closest races. 

# States Poll Count

```{r}
#| label: tbl-state-polls-count
#| tbl-cap: Polls included in Analysis Per State
#| echo: false
#| warning: false

states <- c(state.name, "National")
polls_per_state_tbl <- tibble(state = states)

polls_per_state <- cleaned_data %>% group_by(state) %>% 
  summarise(polls_count = n_distinct(poll_id))

polls_per_state_tbl <- polls_per_state_tbl %>% 
  left_join(polls_per_state, by = "state") %>% 
  mutate(polls_count = if_else(!is.na(polls_count), polls_count, 0)) %>% 
  select(state, polls_count)

polls_per_state_tbl %>% kable(col.names = c("State", "Number of Polls In Analysis"))


num_states_missing <- count(polls_per_state_tbl %>% 
                              filter(polls_count == 0) %>% 
                              select(state))

```

# Trump Voter Prediction Model {#sec-trumpmodel}

The multiple linear regression model (MLR) Bayesian hierarchical model for Donald Trump will use the same variables, formula, and Bayesian approach as the one for Harris. Likewise, the Trump dataset is also split into training and testing data. The model results are similar to that of Harris's MLR model. 

```{r}
#| echo: false
#| warning: false
set.seed(123)
train_data_trump <- read_parquet(here("data/02-analysis_data/train_data_trump.parquet"))
test_data_trump <- read_parquet(here("data/02-analysis_data/test_data_trump.parquet"))
bayesian_model_train_trump <- readRDS(here("models/bayesian_model_train_trump.rds"))
```

```{r}
#| label: fig-model-trumpmlr-res
#| fig-cap: "Bayesian Hierarchical MLR Model of Trump Voting Percentage Captures Variability Without Overfitting"
#| fig-subcap: ["Trump Predictions (Test Data)", "Residuals for Predictions"]
#| echo: false
#| warning: false
#| layout-ncol: 2

predictions2 = posterior_predict(bayesian_model_train_trump, newdata = test_data_trump)
plot(test_data_trump$pct, colMeans(predictions2), 
     xlab = "Actual Voter Percentage Values", 
     ylab = "Predicted Voter Percentage Values")
abline(0, 1, col = "red")

residuals <- test_data$pct - colMeans(predictions)
plot(colMeans(predictions), residuals, 
     xlab = "Predicted Values", 
     ylab = "Residuals")
abline(h = 0, col = "red")
```

Based on @fig-model-trumpmlr-res, the test data predictions are also close to the actual values. This suggests that model has predictability strength. Similarly, the distance between the prediction and the actual values do not appear to follow a pattern, suggesting that the error is due to randomness and not model bias. 

# Trump Model Results {#sec-trumpmodelresults}

```{r}
#| label: fig-trumpmodelci
#| fig-cap: Donald Trump Expected to Recieve Approximatley 45% of the Vote
#| echo: false
#| warning: false
plot(bayesian_model_train_trump, pars = "(Intercept)", prob = 0.95)
```

Based on @fig-trumpmodelci, the model expects Trump to win slight less than 45% of the popular vote and the 95% credible interval ranges from around 42.5% to 47.2%. This credible interval is slightly larger than the model for Harris, implying that the Trump polling data might be less reliable.

```{r}
#| label: fig-model-trumpmlr
#| fig-cap: "Bayesian MLR Trump Model Accounts For A Large Amount of Variability in Voter Percentage"
#| echo: false
#| warning: false
just_trump <- cleaned_data %>% filter(candidate_name == "Donald Trump")
predictions2 = posterior_predict(bayesian_model_train_trump, newdata = just_trump)
predicted_means = colMeans(predictions2)
predicted_intervals = apply(predictions2, 2, quantile, probs = c(0.025, 0.975))
result_summary = data.frame(
  Actual_PCT = just_trump$pct,
  Predicted_PCT = predicted_means,
  Lower_CI = predicted_intervals[1, ],
  Upper_CI = predicted_intervals[2, ]
)
ggplot(result_summary, aes(x = result_summary$Actual_PCT, y = result_summary$Predicted_PCT)) +
  geom_point(alpha = 0.7) +
  geom_errorbar(aes(ymin = Lower_CI, ymax = Upper_CI), width = 0.2, color = "blue") +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  labs(
    title = "Actual vs Predicted Percentage For Trump MLR Model",
    x = "Actual PCT",
    y = "Predicted PCT"
  )
```

```{r}
#| label: tbl-model-trumpmlr
#| tbl-cap: "Head of Bayesian Model Result Summary"
#| echo: false
#| warning: false

kable(head(result_summary, 10), col.names = c("Acutal Harris Percentage", 
                    "Predicted Percentage", 
                    "Lower Bound Credible Interval",
                    "Upper bound Credible Interval"))

```

From @fig-model-trumpmlr and @tbl-model-trumpmlr the model accounts for large amount of the variance in Trump's voter percentage as the data points appear close to the prediction (red line). Furthermore, the distance between the prediction and the actual values do not appear to follow a pattern, suggesting that the error is due to randomness and not model bias.

\newpage

# The Ideal Survey

## Objective 

The research team has developed a survey and distribution methodology with a hypothetical budget of $100,000. This objective is to create a polling system that accurately predicts the 2024 United States Election. 

## Sampling Frame 

Given the monetary constraint, the team's first decision is to mostly include swing states and districts. These are Nevada, Arizona, Wisconsin, Michigan, Pennsylvania, North Carolina, Georgia, Nebraska District 2, and Maine District 2. Swing states are the primary focus of the poll because they disproportionately affect the outcome of the election based on the explanation in @sec-electoral-college. However, the poll will also include other close races like Texas, Florida, and Minnesota, as they have the potential to be won by either party but are not as likely to change as the previously mentioned swing states. Nevertheless, it is important to note that to save costs, the team will focus on polling true swing states compared to states like Texas, Florida, and Minnesota. 

## Survey Sending Process 

The team will then find and use multiple databases, such as U.S. Postal Service’s residential address file [@usps_customer_nodate], to find addresses, telephones, and emails of potential voters in those states. According to @pew_research_center_us_nodate, gathering this information reduces non-response and selection bias, as the team can contact the same individual through multiple mediums. Moreover, certain demographics may prefer specific communication forms; for instance, the elderly may prefer phone or paper mail polls over text or email. Additionally, to encourage survey completion, one in every 50 participants will have a chance at winning $20. 

## Sampling Methodology 

Each state will have its own poll, but the polling methodology and the survey given will remain the same. Specifically, the polls will use stratified random sampling to make sure that participants reflect each state's counties in proportion to their population sizes. County proportion is used as the stratification variable because that information is easily accessible. Additionally, counties possess inherent demographic and cultural characteristics that can influence their alignment with particular political parties. Ideally, stratification would also ensure race, gender, age, and other socioeconomic factors are also accounted for. However, these factors are almost impossible to determine while sending the survey. Therefore, the research team decided to ask demographic questions directly inside the survey. After data collection, the team will weigh data based on demographics. For example, if a certain county has a 30% black population (this statistic will be determined from the US census), but only 15% of the survey participants are black, then the pollsters may decide to count each black participant's responses twice. 

Furthermore, the team will aim to sample approximately 1,000 people from each true swing state. This sample size is large enough to provide reliable conclusions but not so large that it resembles sampling with replacement. For true swing states and other close races, the team aims to survey 1,000 individuals. However, for non-swing states, if the final sample size is less than 1,000, it is not a significant issue. 

## Survey Implementation & Question Creation 

The team's ideal survey will be made using @qualtrics_qualtrics_nodate. It will attempt to ensure four things: no leading questions, no question order bias, no answer order bias, and the survey should identify non-engaged participants. To identify participants who are not engaged, the research team has decided to add a worthless question. This question is extremely simple and clearly has one correct answer. Therefore, if a participant selects the wrong answer, they are not engaged with the survey and their responses should be removed from the final data. An example of this is question number 7. The idea to use a worthless question was inspired by @pew_research_center_us_nodate.

Order bias occurs when the sequence of questions subconsciously influences participants' responses. To prevent this, many questions should be randomized upon entry to the survey. However, some questions must follow a specific order, such as question 11, while others like questions 1-4 can be randomized. The team's hypothetical survey has not implemented this feature, though it is available with the @qualtrics_qualtrics_nodate paid plan. 

Answer bias, like order bias, occurs when the order of answer choices influences participants' selections. For instance, the first option can often be chosen more frequently. To prevent this, answer choices should be randomized upon survey entry. Questions 8 & 9 could benefit from this. Likewise, the team's hypothetical survey has not implemented this feature, but it is available in the @qualtrics_qualtrics_nodate paid plan. 

A leading question occurs when a question is written in a way that suggests the user to give a certain answer. For example, "given that children are the future of our country, should we invest more money in their education". To prevent this, the researchers have ensured questions are written in a style where no unnecessary details or opinions are added. 

## Survey Questions

Click this [link](https://qualtricsxm7d2hxss4j.qualtrics.com/jfe/form/SV_1Tu3PT2eUEa1Op8) for the @qualtrics_qualtrics_nodate survey.

List of all of the questions:

1. Select your race(s) (racial options where chosen based on @orvis_omb_2024)
   - White
   - Black or African American 
   - American Indian or Alaska Native 
   - Native Hawaiian or Pacific Islander 
   - Middle Eastern or North African
   - Asian 
   - Other 
   - Prefer not to answer

2. Please select your gender 
   - Male 
   - Female 
   - Non-binary 
   - Prefer not to say 

3. Please enter your age (in numbers) 
  - This is a text input field. Please note that this field has the auto-validation feature set to numbers in @qualtrics_qualtrics_nodate. As a result, participants can only input numbers in this field and are alert if they have not.

4. Please select the highest degree of education you have obtained
   - GED Certificate 
   - High School Diploma 
   - Undergraduate Degree 
   - Graduate Degree (Masters/Phd) 
   - None 
   - Other
   - Prefer not to answer

5. Are you a registered voter for the 2024 United States Presidential Election? 
   - Yes 
   - No 

6. Place yourself on the political spectrum 
   - Far Left 
   - Center Left 
   - Center 
   - Center Right 
   - Far Right 

7. Can pigs fly? 
   - Yes 
   - Maybe
   - No 
   - Prefer Not To Say 

8. What political party have you registered with? 
   - Republicans 
   - Democrats 
   - Green Party 
   - Libertarian 
   - Other
   - Independent (unregistered) 

9. Who will you vote for in the 2024 presidential election? 
   - Democrat - Kamala Harris 
   - Republican - Donald Trump 
   - Green Party - Gill Stein 
   - Libertarian - Chase Oliver 
   - Other
   - Will not vote

## Potential Problems with the Methodology and Polls 

While the team's methodology and survey create a robust system, there are potential issues. The usage of weighting results based on a candidate's demographics can lead to error propagation. For instance, if a certain racial demographic population is only captured limitedly and that limited sample is far from representative, a few individuals in the population can have major impact on the polls prediction of what candidate will win the state. There is also a selection bias in terms of the monetary reward. Potentially, people who like monetary rewards would be more likely to engage in the survey and could therefore exhibit certain voting characteristics that create an unrepresentative sample.

# Methodology of YouGov 

YouGov's methodology documentation is separated into two articles. The article by @bailey_how_2024 documents the methodology of the 2024 election projection, while the webpage on @yougov_methodology_nodate documents the general methodology of YouGov's prediction. 

## Population, Frame, and Sample 

As @bailey_how_2024 stated, the population covered by YouGov's MRP model is everyone in the national voter file, whether they belong to YouGov's panel. The national voter files are digital databases built by commercial organizations with public government records of voters, as explained by @desilver_q_2018. Voter files indicate whether someone voted in each election, thus YouGov's population covers all voters in previous US elections.  

YouGov’s sampling frame consists of its online panel members. These members are part of the SAY24 project, a collaboration between Stanford, Arizona State, and Yale Universities, as stated by @bailey_how_2024. YouGov collects information on respondents when they join their panel before they are invited to participate in the survey. 

YouGov select the sample from the sampling frame based on their ability to match characteristics of the population of interest. YouGov interviews 100,000 people in the first set of estimates. For the second set of estimates, YouGov did not just start over with a new sample. They took the initial data from August and September and updated it with responses from more than 20,000 additional registered voters who were re-interviewed in late September and early October. 

## Sample Recruitment 

Panelists are recruited through various online channels, including advertisements and partnerships with websites [@yougov_methodology_nodate]. They must provide demographic details upon joining, which helps in selecting representative samples for each survey. When respondents complete a survey, they are awarded points that can be exchanged for money. 

## Sampling Approach and Trade-offs 

YouGov uses non-probability sampling due to the compensation, an approach where not every individual has an equal chance of selection [@yougov_methodology_nodate]. This method allows quick and cost-effective data collection. However, as @yougov_methodology_nodate writes the panelists must have an internet connection to participate. YouGov states that 95% of the US population has internet access, thus the sample may be less representative of certain hard-to-reach populations, such as individuals with slow internet access or without internet access.  

## Non-response Handling 

YouGov applied statistical weighting to adjust the differences between the sample and target population. The weight is based on demographic characteristics such as age, gender, race, and presidential vote [@yougov_methodology_nodate]. Additionally, quality control measures exclude unreliable responses to improve data accuracy. The respondents are offered a small incentive to decrease the non-response and increase participation. 

## Strengths and Weaknesses of the Questionnaire 

YouGov's surveys are conducted online, which is very efficient for the respondents, and responses are weighted to enhance representativeness. The pollster can recruit many panelists because of the online format. Combining with online tracking technologies, the metadata provided by their panelists can be verified easily. 

As a non-probability sample, it might miss certain demographic groups not covered by the online population. While weighting improves accuracy, it cannot fully substitute the randomization found in probability sampling. Additionally, the categories in the survey are oversimplified with bias. For instance, in the poll result published by @yougov_say24_nodate, gender is divided into Male and Female. Race is divided into White, Black, Hispanic, and Other. This indicates a lack of representation.


\newpage


# References


