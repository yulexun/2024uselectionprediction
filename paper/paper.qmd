---
title: "Prediction of 2024 US election ..."
author: 
  - Colin Sihan Yang
  - Lexun Yu
  - Siddharth Gowda
thanks: "Code and data are available at: [https://github.com/yulexun/uselection](https://github.com/yulexun/uselection)."
date: today
date-format: long
abstract: "We forecast the winner of the 2024 US presidential election using “poll-of-polls” by building a linear model."
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(palmerpenguins)
library(here)
library(tibble)
library(knitr)
library(brms)
library(rstanarm)
library(gridExtra)
library(Matrix)
library(rstan)
library(brms)

```

```{r}
#| echo: false
#| warning: false
cleaned_data = read_csv(here("data/02-analysis_data/cleaned_data.csv"))
```

# Introduction {#sec-intro}
Election result forecasting has become an essential tool for analysts in political science and the public to predict the outcome of democratic process, such as the presidential election in the United States. Traditionally, individual polls have been used as a snapshot of voter sentiment, but they only reflect temporary changes in the performance of contestants, instead of a precise estimation of the election result. As discussed by @pasek and @blumenthal, the aggregation of multiple polls, or "poll-of-polls," has become a popular technique to reduce individual survey errors and provide more accurate election forecasts. However, the traditional poll aggregation does not reflect dynamics of an election, especially with real-time changes and the introduction of new data. This creates a gap for a more adaptable model to predict the election result based on both polling data and additional variables, such as historical data and economic indicators.

This paper fills the gap by building a hybrid election forecasting model following the strategies mentioned by @pasek. As @pasek described in their article, aggregation involves determining which surveys are worth including, as well as selecting, combining and averaging results from multiple polls to reduce individual biases and errors. Prediction modeling adds other data to the model that predicts election outcomes based on current dynamics. Hybrid models like the Bayesian approach incorporates prior beliefs based on historical data or expert knowledge and new evidence like economic updates to dynamically adjust the forecast as the campaign progresses. 

In this paper, we aim to predict the 2024 us election result with the hybrid election forcasting model. We incorporate aggregation by filtering the polls on @fivethirtyeight by numeric grade that indicates pollster’s reliability, prediction that incorporates social and economic indicators including unemployment rates and abortion rates, and hybrid approaches that leverages Bayesian techniques which combines historical data such as the 2016 election data, allowing for a dynamic prediction of the U.S. presidential election. 

The estimand for this research paper is the predicted support percentages for Kamala Harris and Donald Trump. The prediction is based on quantifying various polling factors, including sample size, poll scores, and transparency scores, which are used as predictors.

The results of this model indicate a more stable and accurate forecast compared to traditional aggregation methods alone, [update this …]

The remainder of this paper is structured as follows: [update this …]

# Appendix {-}


# Additional data details

# Model details {#sec-model-details}

## Posterior predictive check

In @fig-ppcheckandposteriorvsprior-1 we implement a posterior predictive check. This shows...

In @fig-ppcheckandposteriorvsprior-2 we compare the posterior with the prior. This shows... 

## Diagnostics

@fig-stanareyouokay-1 is a trace plot. It shows... This suggests...

@fig-stanareyouokay-2 is a Rhat plot. It shows... This suggests...

# FiveThirtyEight Licenses
[FiveThirtyEight's data sets](https://github.com/fivethirtyeight/data/tree/master/polls) are used and modified by us under the [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/).

# Trump Voter Prediction Model

The multiple linear regression model (MLR) for Donald Trump will use the same variables, formula, and Bayesian approach as the one for Harris. Likewise, the Trump dataset is also split into training and testing data. The model outputs are below.

```{r}
#| echo: false
#| warning: false
set.seed(123)
train_data_trump <- read_csv(here("data/02-analysis_data/train_data_trump.csv"))
test_data_trump <- read_csv(here("data/02-analysis_data/test_data_trump.csv"))
bayesian_model_train_trump <- readRDS(here("models/bayesian_model_train_trump.rds"))
```

```{r}
#| label: fig-model-trumpmlr
#| fig-cap: "MLR Trump Model Accounts For A Large Amount of Variability in Voter Percentage"
#| echo: false
#| warning: false

just_trump <- cleaned_data %>% filter(candidate_name == "Donald Trump")

predictions2 = posterior_predict(bayesian_model_train_trump, newdata = just_trump)

predicted_means = colMeans(predictions2)
predicted_intervals = apply(predictions2, 2, quantile, probs = c(0.025, 0.975))

result_summary = data.frame(
  Actual_PCT = just_trump$pct,
  Predicted_PCT = predicted_means,
  Lower_CI = predicted_intervals[1, ],
  Upper_CI = predicted_intervals[2, ]
)

# kable(head(result_summary, 10), caption = "Head of Bayesian Model Result Summary")

ggplot(result_summary, aes(x = result_summary$Actual_PCT, y = result_summary$Predicted_PCT)) +
  geom_point(alpha = 0.7) +
  geom_errorbar(aes(ymin = Lower_CI, ymax = Upper_CI), width = 0.2, color = "blue") +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  labs(
    title = "Actual vs Predicted Percentage For Trump MLR Model (All Trump Data)",
    x = "Actual PCT",
    y = "Predicted PCT"
  )
```
From @fig-model-trumpmlr it is clear that the model accounts for large amount of the variance in Trump's voter percentage as the data points appear close to the prediction (red line). Furthermore, the distance between the prediction and the actual values do not appear to follow a pattern, suggesting that the error is due to randomness and not model bias.

```{r}
#| label: fig-model-trumpmlr-res
#| fig-cap: "MLR Trump Model does not Appear to Overfit"
#| echo: false
#| warning: false

predictions2 = posterior_predict(bayesian_model_train_trump, newdata = test_data_trump)

plot(test_data_trump$pct, colMeans(predictions2), 
     xlab = "Actual PCT Values", 
     ylab = "Predicted  PCT Values", 
     main = "Predicted PCT vs Actual PCT (Test Data)")
abline(0, 1, col = "red")  # Add a 45-degree reference line

```
Based on @fig-model-trumpmlr-res, the test data predictions are also close to the actual values. This suggests that model can generalization to outside data. Similarly, the distance between the prediction and the actual values do not appear to follow a pattern, suggesting that the error is due to randomness and not model bias. 



```{r}
#| label: fig-trumpmodelci
#| fig-cap: Donald Trump Expected to Recieve Approximatley 45% of the Vote
#| echo: false
#| warning: false

plot(bayesian_model_train_trump, pars = "(Intercept)", prob = 0.95)
```
Based on @fig-trumpmodelci, the model expects Trump to win slight less than 45% of the popular vote and the 95% confidence interval ranges from around 42.5% to 47.2%. This confidence interval is slightly larger than the model for Harris, implying that the Trump polling data might be less reliable.

# Election Prediction

Our prediction process consists of two primary components. First, we develop models for both Trump and Harris based on the variables outlined in Section @sec-PLACEHOLDER. This involves partitioning the dataset into training and testing subsets. Next, we further divide the testing dataset into swing states and other competitive races. We then input this test data into the respective models to generate predictions. By averaging these predictions, we can calculate the expected voter percentage for each candidate in each state. The candidate with the higher percentage is deemed the winner for that state.

We generated predictions for the following states: Arizona, Nevada, Georgia, Pennsylvania, Michigan, Minnesota, Wisconsin, Florida, Texas, Maine CD-2, Nebraska CD-2, New Hampshire, Ohio, Virginia, North Carolina, and Iowa. Winners for other states were determined based on historical trends and predictions from sources like @cnn. Most states without predictions are strongly Republican or Democratic, so their absence is not expected to significantly impact prediction validity.
```{r}
#| echo: false
#| warning: false
#| label: tbl-electionprediction
#| tbl-cap: Kamala Harris Wins Most of the Swing States
# Split data into training and test sets

train_data_harris <- read_csv(here("data/02-analysis_data/train_data_harris.csv"))
test_data_harris <- read_csv(here("data/02-analysis_data/test_data_harris.csv"))
bayesian_model_train_harris <- readRDS(here("models/bayesian_model_train_harris.rds"))

relevant_states <- c("Arizona", "Nevada", "Georgia", "Pennsylvania", "Michigan", "Minnesota",
                     "Wisconsin", "Florida", "Texas", "Maine CD-2", "Nebraska CD-2", 
                     "New Hampshire", "Ohio", "Virginia", "North Carolina", "Iowa")

# Initialize an empty tibble to store results
swing_state_predictions <- tibble(
  state = character(),
  harris_predicted_pct = numeric(),
  trump_predicted_pct = numeric(),
  winner = character()
)

for(state in relevant_states) {
  #print(state, state %in% unique(test_data_trump), state %in% unique(test_data_harris))
  test_data_state_harris <- test_data_harris[test_data_harris$state == state, ]
  test_data_state_trump <- test_data_trump[test_data_trump$state == state, ]
  
  predictions_state_harris <- posterior_predict(bayesian_model_train_harris, newdata = test_data_state_harris)
  predictions_state_trump <- posterior_predict(bayesian_model_train_trump, newdata = test_data_state_trump)
  
  avg_predicted_pct_state_harris <- mean(predictions_state_harris)
  avg_predicted_pct_state_trump <- mean(predictions_state_trump)
  
  # Add a new row to the results tibble
  new_row <- tibble(
    state = state,
    harris_predicted_pct = avg_predicted_pct_state_harris,
    trump_predicted_pct = avg_predicted_pct_state_trump,
    winner = ifelse(trump_predicted_pct > harris_predicted_pct, "Trump", "Harris")
  )
  # print(state)
  # print(avg_predicted_pct_state_harris)
  # print(avg_predicted_pct_state_trump)
  # 
  swing_state_predictions <- bind_rows(swing_state_predictions, new_row)
}

swing_state_predictions <- swing_state_predictions %>% arrange(state) %>% unique()

# View the results
swing_state_predictions %>% 
  kable(col.names = 
          c("State", "Harris Predicted Percentage", 
            "Trump Predicted Percentage", "State Winner"))
  
```


```{r}
#| label: fig-electionpredictionmap
#| fig-cap: Kamala Harris is Predicited to be the 47th President of the United States
#| echo: false
#| warning: false

# Define your colors for each state (replace these colors with your desired ones)
state_colors_legend <- c(
  "Republican" = "red",      # Dark Red
  "Democrat" = "blue",        # Dark Blue
  "Split By District" = "gray"
)

purple = "purple"
light_blue = "green"
light_red = "orange"

state_colors <- c(
  alabama = "red",
  arizona = ifelse((swing_state_predictions %>% 
                        filter(state == "Arizona") %>% 
                        select(winner)) == "Trump", "red", "blue"),
  arkansas = "red",
  california = "blue",
  colorado = "blue",
  connecticut = "blue",
  delaware = "blue",
  `district of columbia` = "blue",
  florida = ifelse((swing_state_predictions %>% 
                        filter(state == "Florida") %>% 
                        select(winner)) == "Trump", "red", "blue"),
  georgia = ifelse((swing_state_predictions %>% 
                        filter(state == "Georgia") %>% 
                        select(winner)) == "Trump", "red", "blue"),
  idaho = "red",
  illinois = "blue",
  indiana = "red",
  iowa = ifelse((swing_state_predictions %>% 
                        filter(state == "Iowa") %>% 
                        select(winner)) == "Trump", "red", "blue"),
  kansas = "red",
  kentucky = "red",
  louisiana = "red",
  maine = "gray",
  maryland = "blue",
  massachusetts = "blue",
  michigan = ifelse((swing_state_predictions %>% 
                        filter(state == "Michigan") %>% 
                        select(winner)) == "Trump", "red", "blue"),
  minnesota = ifelse((swing_state_predictions %>% 
                        filter(state == "Minnesota") %>% 
                        select(winner)) == "Trump", "red", "blue"),
  mississippi = "red",
  missouri = "red",
  montana = "red",
  nebraska = "gray",
  nevada = ifelse((swing_state_predictions %>% 
                        filter(state == "Nevada") %>% 
                        select(winner)) == "Trump", "red", "blue"),
  `new hampshire` = ifelse((swing_state_predictions %>% 
                        filter(state == "New Hampshire") %>% 
                        select(winner)) == "Trump", "red", "blue"),
  `new jersey` = "blue",
  `new mexico` = "blue",
  `new york` = "blue",
  `north carolina` = ifelse((swing_state_predictions %>% 
                        filter(state == "North Carolina") %>% 
                        select(winner)) == "Trump", "red", "blue"),
  `north dakota` = "red",
  ohio = ifelse((swing_state_predictions %>% 
                        filter(state == "Ohio") %>% 
                        select(winner)) == "Trump", "red", "blue"),
  oklahoma = "red",
  oregon = "blue",
  pennsylvania = ifelse((swing_state_predictions %>% 
                        filter(state == "Pennsylvania") %>% 
                        select(winner)) == "Trump", "red", "blue"),
  `rhode island` = "blue",
  `south carolina` = "red",
  `south dakota` = "red",
  tennessee = "red",
  texas = ifelse((swing_state_predictions %>% 
                        filter(state == "Texas") %>% 
                        select(winner)) == "Trump", "red", "blue"),
  utah = "red",
  vermont = "blue",
  virginia = ifelse((swing_state_predictions %>% 
                        filter(state == "Virginia") %>% 
                        select(winner)) == "Trump", "red", "blue"),
  washington = "blue",
  `west virginia` = "red",
  wisconsin = ifelse((swing_state_predictions %>% 
                        filter(state == "Wisconsin") %>% 
                        select(winner)) == "Trump", "red", "blue"), 
  wyoming = "red")

# Load the states map data
states_map <- map_data(map = "state")

state_colors <- setNames(state_colors, tolower(names(state_colors)))

color_to_status <- setNames(names(state_colors_legend), state_colors_legend)

states_map <- states_map %>%
  mutate(status = color_to_status[state_colors[region]])

plotclr <- c("red", gray, "blue")
legend_description <- c(
  "Republican",
  "Democrat",
  "Split by District"
)


ggplot(states_map, aes(x = long, y = lat, group = group, fill = status)) +
  geom_polygon(color = "white", size = 0.2) +
  coord_map("albers", lat0 = 39, lat1 = 45) +
  scale_fill_manual(
    values = state_colors_legend,
    name = "Election Status"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    legend.title = element_text(size = 12, face = "bold"),
    legend.text = element_text(size = 10),
    axis.text = element_blank(),
    axis.title = element_blank(),
    panel.grid = element_blank()
  )

```
According to Figure @fig-currentstateofelection, Kamala Harris is predicted to be the 47th President of the United States. There are also a few states with predictions not visible in the map, we will describe those predicts them below 

In Maine, Harris is projected to win the state’s overall delegates and District 1, while District 2 is expected to go to Trump. In Nebraska, Trump is expected to win the state’s delegates along with Districts 1 and 3, while Harris is predicted to win District 2. Additionally, Trump is projected to win Alaska, and Harris is expected to win Hawaii.

Overall, the predictions indicate that Harris will receive 298 delegates, while Trump will receive 240 delegates.

\newpage


# Sources of Bias In the Polls

Based on the figures in section @sec-data, the team identified three sources of bias beyond sample size and state: methodology, the timing of polls relative to the election date, and poll scores. From the analysis in Figure @fig-methodologyboxplot, we found that polls employing a limited number of methods to reach and communicate with voters exhibited greater variability. This finding is plausible, as different demographic groups often prefer distinct communication methods; for example, older voters may favor phone surveys, while younger voters might prefer online surveys. Additionally, utilizing multiple communication channels provides pollsters with more opportunities to contact potential voters, thereby reducing non-response bias. Consequently, it is essential to incorporate methodology as a variable in the model, and we recommend that pollsters adopt diverse communication methods whenever possible.

From Figure @fig-daysfromelection-pct, the team observed a decline in third-party candidate support as the election approached. Early in the election cycle, voters may entertain the idea of a viable third-party candidate, but as the election nears, interest in third-party options diminishes. This trend may be worsened by decreased participation in polls as the election date approaches. As a result, the people who do participate in those earlier polls are likely to be more informed and passionate, which are qualities that may be associated with support for third-party candidates. Additionally, the withdrawal of a prominent third-party candidate, Robert F. Kennedy Jr., on August 23, 2024 (74 days from the election), could influence this dynamic. However, we find the earlier reasons more compelling, as the data in Figure @fig-daysfromelection-pct do not indicate a significant drop in third-party support at that time, but rather a gradual decline.

As illustrated in Figure @fig-pollscore-pct, more reliable polls tend to show a bias favoring Trump over Harris—not necessarily indicating a guaranteed victory for Trump, but rather suggesting he may outperform expectations in many polls. It is important to note that the 538 poll score system is partly based on historical accuracy. Given Trump's overperformance in the polls during the 2016 and 2020 elections, this scoring may be overcorrecting for past inaccuracies. This potential undervaluation of Trump’s support could be strategically beneficial for the Harris campaign, providing a warning to avoid repeating previous electoral mistakes.

## Limitations of the Model

While this model presents a more comprehensive approach, there are limitations. First, Any systematic biases inherent in those polls can carry over into the model’s predictions because we are using existing poll data. For instance, if certain demographic groups are consistently underrepresented, our model may not fully capture their impact on election outcomes. 

Second, the model only reflect current voting dynamics as it is based on historical data. This model does not account for short-term variability such as Biden's exit. This limitation is important in the final weeks before an election when small shifts in polling data can produce exaggerated effects.

On top of that, the results can be sensitive to the choice of priors. We choose normal(0, 2.5) as we believe it could stabilize the estimates by being informative enough to guide the posterior distribution without overpowering the data. However, if the chosen prior does not align well with the true underlying distribution, it may lead to biased results or overly wide credible intervals.

Also, as it is a model based on a dataset, it is challenging to capture all the real-life features and dynamics. The model might not include all relevant predictors or interactions due to limitations in data availability or complexity, which can lead to incomplete representations of the factors influencing voting behavior. Additionally, without strong regularization techniques, the model may become prone to overfitting, particularly when using complex hierarchical structures or including numerous predictors. This overfitting can reduce the model’s generalizability to new or unseen data.

Moreover, errors and offsets inherent in polling data, such as response bias, nonresponse adjustments, and sampling variability, can propagate into the model’s results. These aspects introduce an additional layer of uncertainty that can affect the model's reliability and predictive performance. While Bayesian methods provide a robust framework to incorporate uncertainty, the final outputs must be interpreted cautiously, acknowledging these underlying limitations.



# Limitations of the Prediction

In section @sec-prediction, the team predicted that Kamala Harris would secure 298 delegates while Donald Trump would obtain 240 delegates. However, this prediction has several limitations. Firstly, the model used to generate these predictions inherits the constraints outlined in section @sec-model-limitations. Additionally, the predictions are based on averaging results from polling data for each state, which can lead to propagated prediction errors.

Each state has a limited number of polls available; while the overall analysis considered a substantial number of polls, individual states often had fewer than 20 polls, with even the largest states contributing only up to 27. Moreover, as noted in Section @sec-prediction, the analysis focused solely on closely contested races, specifically swing states. Consequently, many states were assigned to a candidate based on strong historical trends rather than direct predictions. Although this approach is reasonable, it may overlook emerging trends that could potentially shift a state's allegiance from Republican to Democrat or vice versa.




# References
